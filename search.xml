<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CS285 Fa19 Introduction to Reinforcement Learning]]></title>
    <url>%2F2020%2F03%2F08%2F2020-02-25-cs285-4%2F</url>
    <content type="text"><![CDATA[CS 285 at UC Berkeley을 보고 정리한 글입니다. Definitions 강화학습의 목적은 $\theta$를 찾는 것이 목적이다. state는 markov property를 만족하고 observation은 만족하지 않는다. $\pi_{\theta} \mathbf (a_t|o_t)$는 partially observed라고 하고 $\pi_{\theta} \mathbf (a_t|s_t)$ fully observed라 한다. POMDP(Partially observable Markov decision process)와 Fully observed MDP가 있는데 일반적으로 Fully observed MDP로 가정한다. behavior cloning 전문가로부터 data(observation,action)을 수집해서 supervised learning을 사용한다. 일반적으로 behavior cloning은 잘 안되지만 DAgger를 사용하면 성능이 향상된다. reward function $\mathbf r(s,a)$(reward function): 어떤 행동이 좋고, 어떤 행동이 나쁜가?에 대한 기준 $s,{\mathbf a},r(s,a),p(s’|s,a)$ (transition probability)로 이뤄진 것을 Markov decision process라 한다. Markov chain $\mathcal{M} = {\mathcal{S,T}}$ 아직 RL은 아니다.(no agency,no action) $\mathcal{S}$ - state space states $s \in \mathcal{S}$(discrete or continuous) $\mathcal{S}$: set of valid state $s$:state $\mathcal{T}$ - Transition operator $p (s_{t+1}|s_t)$ why “operator”? Discrete space에서 $\mathcal{T}$ linear operator처럼 쓰이기 때문 let $\mu_{t,i} = p(s_t = i) \vec{\mu_t}$ is a vector of probabilites $\mu_{t,i}$: time step $t$에서 주어진 state $i$일 확률 let $\mathcal{T}_ {i,j} = p(s_{t+1} = i|s_t = j)$ then $\vec{\mu_{t+1}} = \mathcal{T}\vec{\mu_t}$ $\mathcal{T}_{i,j}$ -&gt; $N\times N$ matrix($N$:가능한 state 개수) 다음 step의 state 확률을 $\mathcal{T}$와 현재 step의 state 확률의 곱으로 구할 수 있다. $\vec{\mu_{t+1}} = \mathcal{T}\vec{\mu_t}$이 linear operator 처럼 행동 stationary distribuition 때 중요 Markov decision process $\mathcal{M} = {\mathcal{S,A,T,r}}$ $\mathcal{S}$ - state space states $s \in \mathcal{S}$(discrete or continuous) $\mathcal{A}$ - action space actions $a \in \mathcal{A}$(discrete or continuous) $\mathcal{O}$ - observation space observations $o \in \mathcal{O}$(discrete or continuous) $\mathcal{T}$ - Transition operator(Tensor!) action과 state를 가지고 있기에 Tensor이다.($p (s_{t+1}|s_t,a_t)$) let $\mu_{t,j} = p(s_t = j)$ let $\xi_{t,k} = p(a_t = k)$ let $\mathcal{T}_ {i,j,k} = p(s_{t+1} = i|s_t=j,a_t=k)$ $\mu_{t+1,i} = \sum_{j,k} \mathcal{T}_ {i,j,k}\mu_{t,j}\xi_{t,k}$(linear operator) $r$ - reward function $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$(scalar field) $r(s_t,a_t)$ - reward Partially observed Markov decision process $\mathcal{M} = {\mathcal{S,A,O,T,\varepsilon,r}}$ $\mathcal{S}$ - state space states $s \in \mathcal{S}$(discrete or continuous) $\mathcal{A}$ - action space actions $a \in \mathcal{A}$(discrete or continuous) $\mathcal{O}$ - observation space observations $o \in \mathcal{O}$(discrete or continuous) $\varepsilon$ - emission probability(operator) $p(o_t|s_t)$ $r$ -reward function $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ The goal of reinforcement learning Goal: reward를 최대로하는 policy 찾기(learn $\theta$) 항상 policy가 explicit 하지 않지만 지금은 explicit $p_\theta(s_1,{\mathbf a_1},\dots,s_T,{\mathbf a_T}) = p(s_1) \prod_{t=1}^T\pi_\theta({\mathbf a_t}|s_t)p(s_{t+1}|s_t,{\mathbf a_t})$ finite horizon으로 가정 $p_\theta(s_1,{\mathbf a_1},\dots,s_T,{\mathbf a_T}) = p_\theta(\mathcal{T})$ (joint probability distribution, $\mathcal{T}$는 trajectory) $s_{t+1}$는 보통 모른다. $\theta^\star = \arg \max_{\theta}E_{\mathcal{T}\sim p_\theta(\mathcal{T})}[\sum_t r(s_t,{\mathbf a_t})]$ 우리는 모든 time step마다 reward가 최대가 되길 원한다. 그래서 모든 time step에 대해 reward를 더한다. $s_t,{\mathbf a_t}$는 $\mathcal{T}\sim p_\theta(\mathcal{T})$에 대해 uncertain(randomly distributed)하기에 기댓값을 사용($\sum_{x-p(x)}[f(x)]=\int p(x)f(x)dx)$) 이 식은 policy의 목적이기에 action을 알려주지 않아서 기댓값은 가능한 모든 state와 action의 sequence를 고려해야한다. $\theta = \arg \max_{\theta}E_{\mathcal{T}\sim p_\theta(\mathcal{T})}[\sum_t r(s_t,{\mathbf a_t})]$ distribution이 MDP를 만족하지 않아도 $\pi_\theta({\mathbf a}|s)$가 정해지면 $s$와 $\mathbf a$의 합쳐짐으로 얻어진 augmented state space에서 markov chain으로 볼수있다. $p((s_{t+1},\mathbf{a_{t+1}})|(s_t,\mathbf{a_t}))) = p(s_{t+1}|s_t,\mathbf{a_t})\pi_\theta(\mathbf{a_{t+1}}|s_{t+1})$ Finite horizon case: state-action marginal $\theta^\star = \arg \max_{\theta}E_{\mathcal{T}\sim p_\theta(\mathcal{T})}[\sum_t r(s_t,{\mathbf a_t})]$ $\mathcal{T}$를 $s$와 $a$의 pair로 변경 $p_\theta(s_t,\mathbf{a}_t)$(state-action marginal) time step $t$에서의 state와 action의 marginal distribution Infinite horizon case: stationary distribution what if $T = \infty$? $p(s_t,\mathbf{a_t})$가 stationary distribution으로 수렴 한다면(항상은 아니지만 일반적으로 수렴) $\begin{pmatrix} s_{t+1} \ a_{t+1} \end{pmatrix} = \mathcal{T}^k \begin{pmatrix} s_{t} \ a_{t} \end{pmatrix}$(state-action transition operator)을 이용해서 $\mu = \mathcal{T}\mu$로 나타낼 수 있다. $\mu$: vector of probabilities state action pair $\mu = \begin{bmatrix} p(s_1) \cr p(s_2) \cr \vdots \cr p(s_N) \end{bmatrix} \ \ \text{(regular markov chain) } \mu = \begin{bmatrix} p(s_1,a_1) \cr p(s_1,a_2) \cr p(s_1,a_M) \cr p(s_2,a_1) \cr \vdots \end{bmatrix} \ \ ( \text{markov chain over} s \ \ \text{and} \ \ a)$ not change distribution $\ne$ not change state $(\mathcal{T} - I)\mu = 0$ $\mu$는 $\mathcal{T}$의 eigenvalue가 1인 eigenvector이다. 특정 조건(ergodicity - 모든 state action pair가 reachable)을 만족하면 수렴한다. $\theta^\star = \arg \max_{\theta} \frac{1}{T} \sum^T_{t=1}E_(s_t,\mathbf{a_t})\sim p_\theta(s_t,\mathbf{a_t})[r(s_t,\mathbf{a_t})] \rightarrow E_{(s,\mathbf{a})\sim p_\theta(s,\mathbf{a})}[r(s,\mathbf{a})]$ $T$가 무한대으로 가면 이전 공식이 무한대로 가기 때문에 $\frac{1}{T}$을 추가함(undiscounted average return) $\sum$이 $E$에 압도되어 결국 $E_{(s,\mathbf{a})\sim p_\theta(s,\mathbf{a})}[r(s,\mathbf{a})]$이 된다. RL에서 목적은 주로 expectation만을 고려한다. 그림처럼 도로위에 있으면 reward +1 이고 그외에는 -1이라 하자 reward function은 not smooth, not differentiable하다. 하지만 expectation을 추가하면 $\theta$ ($pi_\theta(\mathbf{a}=fall)=\theta$, distribution) 안에서는 smooth 해진다.($\theta * (-1) + (1-\theta)*1$이므로) AlgorithmsThe anatomy of RL algorithm ( sample 생성(즉 policy대로 agent 행동, collect data) ) ( Fit a model/ estimate the return(evaluation step, policy를 바꾸지 않고 평가한다.) ) ( improve the policy(policy를 업데이트한다.) ) ( 위 그림에서 trajectories ) ( evaluate reward function(나쁜결과는 probability 낮춘다)) ( $\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta)$(gradient descent) ) Model based RL ( $s_t$와 $\mathbf a_t$가 input으로 받고 output으로 $s_{t+1}$를 출력하는 $f_{\phi}$를 학습한다.(fitting neural net) ) ( $\pi_\theta(s_t)$를 학습 시키기 위해 $f_{\phi}$와 $r$을 통해서 backprop한다. ) Which parts are expensive? ( 실제 로봇이나 차등은 현실 시간과 동일하므로 오래 걸리지만 MuJoCo같은 시뮬레이션은 빠르다. ) ( reward를 계산하는 것은 빠르지만 neural net을 사용하면 느리다.(알고리즘에 따라 다르다.) ) ( gradient step같은경우 빠르지만 backprop은 느리다.(알고리즘에 따라 다르다.) ) How do we deal with all these expectations? $$E_{\mathcal{T}\sim p_\theta(\mathcal{T})}[\sum_t r(s_t,\mathbf a_t)]$$ $ E_{s_1 \sim p(s_1)} [E_{\mathbf{a}_1\sim \pi (\mathbf {a} _1|s_1)} [r(s_1,{\mathbf a}_1)+\ |s_1]] $ state $s_1$에서 action $a_1$을 한다. $ E_{s_{1} \sim p\left(s_{1}\right)}\left[E_ {\mathbf{a}_ {1} \sim \pi\left(\mathbf{a}_ {1} | \mathbf{s}_ {1}\right)}\left[r\left(\mathbf{s}_ {1}, \mathbf{a}_ {1}\right)+E_{\mathbf{s}_ {2} \sim p\left(\mathbf{s}_ {2} | \mathbf{s}_ {1}, \mathbf{a}_ {1}\right)}[ | \mathbf{s}_ {1}, \mathbf{a}_ {1}\right] | \mathbf{s}_{1}]\right] $ transition probability distribution $p(s_2|s_1,\mathbf{a_1})$에 따라 state $s_2$로 가고 action $a_2$를 한다. $$ E_{s_{1} \sim p\left(s_{1}\right)}\left[E_ {\mathbf{a}_ {1} \sim \pi\left(\mathbf{a}_ {1} | \mathbf{s}_ {1}\right)}\left[r\left(\mathbf{s}_ {1}, \mathbf{a}_ {1}\right)+E_{\mathbf{s}_ {2} \sim p\left(\mathbf{s}_ {2} | \mathbf{s}_ {1}, \mathbf{a}_ {1}\right)}\left[E_{\mathbf{a}_ {2} \sim \pi\left(\mathbf{a}_ {2} | \mathbf{s}_ {2}\right)}\left[r\left(\mathbf{s}_ {2}, \mathbf{a}_ {2}\right)+\ldots | \mathbf{s}_ {2}\right] | \mathbf{s}_ {1}, \mathbf{a}_ {1}\right] | \mathbf{s}_{1}\right]\right] $$ expectation이 많지만 markov property때문에 각각 1,2개의 condition만을 가진다.(convenient 해진다.) ex) $E_ {\mathbf{a}_ 2 \sim \pi (\mathbf{a}_ {2} | \mathbf{s}_{2})}$는 $s_2$만을 condition으로 받음 $ r (\mathbf{s}_ {1}, \mathbf{a}_ {1} )+E_{\mathbf{s}_ {2} \sim p (\mathbf{s}_ {2} | \mathbf{s}_ {1}, \mathbf{a}_ {1} )} [E_{\mathbf{a}_ {2} \sim \pi (\mathbf{a}_ {2} | \mathbf{s}_ {2} )} [r (\mathbf{s}_ {2}, \mathbf{a}_ {2} )+\ldots | \mathbf{s}_ {2} ] | \mathbf{s}_ {1}, \mathbf{a}_ {1} ] | \mathbf{s}_{1} ]]$를 안다면? time step $T$까지 기다릴 필요 없이 바로 policy를 앞으로의 보상에 맞게 학습할 수 있을 것이다.(drastically simplify) $Q(s_1,\mathbf{a_1}) = r(\mathbf{s}_ {1}, \mathbf{a}_ {1})+E_{\mathbf{s}_ {2} \sim p(\mathbf{s}_ {2} | \mathbf{s}_ {1}, \mathbf{a}_ {1})}[E_{\mathbf{a}_ {2} \sim \pi(\mathbf{a}_ {2} | \mathbf{s}_ {2})}[r(\mathbf{s}_ {2}, \mathbf{a}_ {2})+\ldots | \mathbf{s}_ {2}] | \mathbf{s}_ {1}, \mathbf{a}_ {1}]$ $E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^{T} r\left(\mathbf{s}_ {t}, \mathbf{a}_ {t}\right)\right]=E_{\mathbf{s}_ {1} \sim p\left(\mathbf{s}_ {1}\right)}\left[E_{\mathbf{a}_ {1} \sim \pi\left(\mathbf{a}_ {1} | \mathbf{s}_ {1}\right)}\left[Q\left(\mathbf{s}_ {1}, \mathbf{a}_ {1}\right) | \mathbf{s}_{1}\right]\right]$ 만약 모든 $s$, $a$에 대해서 $Q(s,a)$를 안다면 $\pi$를 쉽게 바꿀 수 있다. ex) 만약 ${\mathbf a_1} = \arg \max_{\mathbf a_1} Q(s_1,{\mathbf a_1})이면 $\pi_\theta({\mathbf a_1}|s_1) = 1$ 이다.(나머지 확률이 0이라) $Q$도 $\pi$에 영향을 받지만 지금은 고려하지 않는다.(나중에 트릭으로 해결) Q-function and value function Q-function(quality function): $s_t$에서 $\mathbf a_t$를 했을때 앞으로 받을 전체 reward의 합 $Q^{\pi}(s_t,{\mathbf a_t}) = \sum\nolimits^T_{t’=t} E_{\pi_\theta}[r(s_{t’},{\mathbf a}_{t’})|s_t,{\mathbf a_t}]$ value function: $s_t$로부터 앞으로 받을 전체 reward의 합 $V^{\pi}(s_t) = \sum\nolimits_{t’=t}^{T} {E}{\pi{\theta}} [ r(s_{t’},{\mathbf a}_{t’}) | s_t]$ $V^{\pi}\left(\mathbf{s}_ {t}\right)=E_{\mathbf{a}_ {t} \sim \pi\left(\mathbf{a}_ {t} | \mathbf{s}_ {t}\right)}\left[Q^{\pi}\left(\mathbf{s}_ {t}, \mathbf{a}_{t}\right)\right]$ Idea1: $\pi$와 $Q^\pi(s,{\mathbf a})$를 안다면 $\pi$를 improve 할수있다. 만약 ${\mathbf a} = \arg \max_{\mathbf a} Q^\pi(s,{\mathbf a})라면 ${\pi}’({\mathbf a}|s) = 1$으로 맞춘다. 그러면 ${\pi}’$는 적어도 $\pi$보다 좋거나 같아진다. Idea2: good action $\mathbf a$의 확률 증가시키기 위해 gradient 계산 만약 $Q^\pi(s,{\mathbf a}) &gt; V^{\pi}(s)$면 $\mathbf a$는 평균보다 더 좋다.($V^{\pi}(s_t) = \sum\nolimits_{t’=t}^{T} {E}_ {\pi_{\theta}} [ r(s_{t’},{\mathbf a}_{t’}) | s_t]$이기 때문) $Q^\pi(s,{\mathbf a}) &gt; V^{\pi}(s)$인 $\mathbf a$의 확률을 크게하게 $\pi({\mathbf a}|s)})$로 바꾼다.(actor critic) Types of RL algorithms Objective: $\theta^\star = \arg \max_{\theta}E_{\mathcal{T}\sim p_\theta(\mathcal{T})}[\underset{t}\sum r(s_t,\mathbf a_t)]$ Policy gradients: 위 objective를 집접적으로 미분 Value-based: optimal policy의 Q-function or value function을 추정(no explicit policy) Actor-critic: 현재 policy의 Q-function or value function을 추정, policy를 향상시키기 위해 사용(combination Policy gradients and Value-based) Model-based RL: transition model 추정, 그리고 planning을 위해 모델사용(no explicit policy) policy 향상을 위해 사용 Something else plan하기 위해 모델 사용(no explicit policy) trajectory 최적화/ 최적 제어(우선적으로 continuous space) - 필수적으로 action을 최적화하기 위해 backpropagation discrete action space에서 discrete planning - e.g., Monte Carlo tree search Backpropagate gradient into the policy 동작하기 위해 몇가지 트릭 필요 Value function을 배우기 위해 모델 사용 Dynamic programming Generate simulated experience for model-free learner(Dyna) TradeoffsWhy so many RL algorithms Different tradeoffs Sample efficiency(sample collection time) stability &amp; ease of use(reliable) Different assumptions Stochastic or deterministic? Continuous or discrete? Episodic or infinite horizon? Different things are easy or hard in different settings Easier to represent the policy?(enviornment의 물리법칙이 복잡하지만 optimal behavior의 패턴은 단순) Easier to represent the model?(ex. chess) sample efficiency Sample efficiency = 좋은 정책을 얻기 위해서 얼마나 많은 샘플이 필요한가? 가장 중요한 질문: Off policy 알고리즘 인가? Off policy: policy에서부터 새로운 샘플 생성없이 policy를 향상할 수 있다. On policy: policy가 조금이라도 변할 때마다 새로운 sample을 생성해야 한다. 왜 덜 efficient한 알고리즘을 사용하는가? simulation power가 적게 들면, less efficient하더라도 더 나은 성능을 위해 많은 sample을 얻어서 학습하는 것이 효율적일 수 있다. stability and ease of use 수렴하는지, 어디에 수렴하는지, 항상 수렴하는지에 대한 이슈다. supervised learning에서는 gradient descent를 사용하기 때문에 대개 수렴하지만, RL에서는 대체로 gradient descent를 사용하지 않는다. Q-learning: fixed point iteration. converge가 보장되지 않는다. Model-based RL: model이 reward에 대해 최적화되는 것이 아니다. Policy gradient: gradient descent 보통 least efficient .(but stability) Value function fitting 가장 좋을때, error of fit을 최소화한다.(Bellman error) expected reward와 같지 않다.(better fit for value function $\ne$ better policy) 최악의 경우, 최적화 되지않는다. 많은 유명한 deep RL value fitting 알고리즘은 비선형의 경우 converge한다고 장담할 수 없다. 직접적으로 expected reward를 최적화 하지않는다. Model-based RL model이 error of fit를 최소화한다. 수렴한다. better mode = better policy라 보장할 수 없다. Policy gradient 유일하게 실제로 true objective를 gradient descent(ascent)를 사용한다. assumptions full observability 일방적으로 value function fitting에서 가정된다. recurrence를 추가하여 완화할 수 있다. episodic learning 주로 pure policy gradient에서 가정된다. 몇가지 model-based RL에서 가정된다. continuity or smoothness 몇가지 continuous value function learning 방법에서 가정된다. 몇가지 model-based RL에서 가정된다.]]></content>
      <categories>
        <category>lecture</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS285 Fa19 Deep Reinforcement Learning, Decision Making, and Control]]></title>
    <url>%2F2020%2F02%2F25%2F2020-02-25-cs285-1%2F</url>
    <content type="text"><![CDATA[CS 285 at UC Berkeley을 보고 정리한 글입니다. Terminology &amp; notation image classification에서는 input으로 image로 받고 output으로 class이지만 순차적 의사결정 문제(sequential decision making problem)에서는 아래첨자에 time step $t$가 추가 되고 output이 $\bf a_t$(action)이 된다. 그리고 순차적 의사결정 문제는 action이 observation($\bf o_t$)에 영향을 끼친다. $\pi_{\theta} \bf (a_t|o_t)$(policy)은 $o_t$가 주어졌을 때 $a_t$의 확률분포이다. observation 과 state의 차이observation($o_t$)는 input 이미지로 볼 수 있고 state($\bf s_t$)는 치타의 상태와 가젤의 상태(environment의 모든 정보)라 할 수 있다. 그래서 위 그림같이 이미지에서 자동차가 치타를 가리면 observation에서는 치타의 정보를 알 수 없지만 state는 알 수 있다.($\pi_{\theta} \bf (a_t|o_t)$는 partially observed라고 하고 $\pi_{\theta} \bf (a_t|s_t)$ fully observed라 한다.) $\bf p(s_{t+1}|s_t,a_t)$: state $\bf s_t$에서 action $\bf a_t$를 취했을때 $\bf s_{t+1}$로 갈 확률 이러한 성질은 markov chain에서 보면 위 그림과 같다. 여기서 $\bf s_t$에서 $\bf s_{t+1}$로 갈때 $\bf s_{t-1}$은 이와 독립적이다.(future은 past에 독립) 이를 Markov property라고 한다. 위와 같은 그래프가 형성되기 위해서는 state가 필요한데 그 이유는 observation은 markov property를 만족하지 않기 때문이다. ex) 위 치타와 차 예시를 보면 $\bf o_{t-1}$를 차가 치타를 가리기 전 $\bf o_t$를 차가 치타를 가린 경우 $\bf o_{t+1}$를 차가 지나간 후라면 $\bf o_{t+1}$는 $o_{t-1}$의 정보를 알지 못하면 알 수 없기에 markov property를 따르지 않는다. 가끔 state를 $\scr{x_t}$로 action을 $\bf u_t$로 나타내기도 한다. Imitation Learning 사람(전문가)으로부터 observation $\bf o_t$와 action $\bf a_t$를 모아서 $\pi_\theta \bf (a_{t}|o_{t})$를 supervised learning 하는 것을 behavior cloning이라 한다. 검은선: training data의 trajectory 빨간선: 학습시킨 $\pi_{\theta}$의 trajectory 일반적으로 이러한 방법은 잘 안되는데 그 이유는 supervised learning이 정확하게 training data와 100% 똑같지 않기때문에 초반에 mistake가 일어나고 그 결과 모르는 trajectory로 가게 되고 계속해서 mistake가 커지기 때문이다. 문제 개선 End to End Learning for Self-Driving Cars 에서 위에서 나온 문제를 해결하기 위해 차에 3개의 카메라를 달아서 해결하였다. 왼쪽 카메라는 오른쪽 핸들로 bias를 오른쪽 카메라는 왼쪽 핸들로 bias를 주어 상호보완이 가능하게 하여 학습이 더 잘되게 하였다. 하지만 이 방법은 자율주행에서만 가능하다. trajectory의 distribution 안정적이게 되었다. $\bf p_{\text {data}}(o_t)$: training data(observation)의 distribution$\bf p_{\pi_\theta}(o_t)$: $\pi_\theta$를 따랐을때 나온 observation 의 distribution 실제 training data는 $\bf p_{\text {data}}(o_t)$에서 sampling된 것으로 볼 수 있다.여기서 $o_t$는 서로 독립적이지 않지만, supervised learning을 이용하기에 independent가 된다. 여기서 문제는 $\bf p_{\text {data}}(o_t)$와 $\bf p_{\pi_\theta}(o_t)$가 달라서 mistake가 생기게 된다는 것이다. 그렇다면 $\bf p_{\text {data}}(o_t) = p_{\pi_\theta}(o_t)$로 강제로 만들어 주면 어떻게 될까? DAgger 여기서 우리는 policy를 data만큼 바꾸지 않을 것이다. DAgger의 idea는 training datg를 $\bf p_{\text {data}}(o_t)$가 아닌 $\bf p_{\pi_\theta}(o_t)$에서 모으는 것이다. human data로 $\bf \pi_\theta (a_t|o_t)$를 학습 시킨다. $\bf \pi_\theta (a_t|o_t)$로 새로운 데이터($o_t$)를 얻는다. 사람이 새로운 데이터에 라벨($\bf a_t$)을 달아준다. 새로 얻은 데이터를 training data에 합친다. 1-4를 계속 반복한다. 하지만 사람이 label을 정의 해주기 힘들고 너무나 많은 노동력이 든다는 문제점이 생긴다. 만약 모델이 drift가 생기지 않을 정도로 학습이 잘되고 overfit 되지 않는다면 성능이 올라갈 것이다. 하지만 그렇게는 잘되지 않는다. 실패 원인Non-Markovian behavior $\bf \pi_\theta (a_t|o_t)$는 현재 observation에만 의존하여 행동하지만, 실제 사람은 과거의 observation도 고려하기에 문제가 발생하였다. 위 문제의 해결방안중 하나는 RNN이다. Multimodal behavior 파란색 바그래프는 discrete case를 나타낸 것이고 밑에는 continuous 정규분포(Maximum Likelihood)로 나타낸 것이고 이는 적용되지않는다. 위 나무 그림처럼 같은 상황에서도 다양한 행동(왼쪽, 오른쪽)이 가능하다. 이러한 문제는 Output Mixture of Gaussians, Latent variable model, Autoregressive discretization으로 해결할 수 있다. Output mixture of Gaussians action을 선택할 때 한 개의 Gaussian에서 뽑는 것이 아닌 여려 개의 Gaussian을 이용하는 방법이다.하지만 이 방법은 n개의 action만 사용 가능 하므로 action이 많을수록 사용하기 힘들다는 단점이 있다. latent variable models 이 방법은 input에 노이즈를 추가해서 딥러닝 모델에 학습시킨다. 원하는 대로 표현이 가능하지만, 훈련이 어렵다는 단점이 있다. autoregressive discretization softmax를 통해 discrete한 확률분포를 뽑아내고 분포에서 sampling하여 또 다른 네트워크에 입력으로 넣어주고 다시 확률분포를 뽑아준다. 이 과정을 n번 반복하여 n개의 dimension을 출력으로 뽑아내게 된다.이 방법은 만약 작은 차원의 action space를 가지고 있고 action space가 continuous이라도 discrete하게 바꿀 수 있다. 하지만 high dimential에서는 practical 하지 않다. Imitation learning: recap Behavior cloning은 종종 이 방법만으로 불충분하다. 왜냐하면, distribution mismatch problem이 발생하기 때문이다. 하지만 다음과 같은 방법을 쓰면 잘 작동한다. Hacks (e.g 자율주행에서 카메라 3개를 쓴 방법) 안정적인 trajectory distribution으로부터 sampling on-policy data를 추가한다. (e.g Dagger 사용) 더 정확하게 모델링 한다. Imitation learning의 한계 사람이 data를 제공해야 한다. 딥러닝은 data가 많을 때 잘된다. 사람이 특정 action을 정해주기 힘든 경우도 있다. 사람은 자동으로 배우는데 기계는 그렇지 못한다. 스스로 경험으로부터 무제한 데이터가 필요 연속적으로 스스로 improvement 해야 한다. learning without imitation $\delta$: 사자한테 먹히면 1 아니면 0 사자예시로 다시 돌아가보면 우리는 사자에게 먹히지 않는 것이 목표이다.(위 식의 기댓값을 minimize) 우리는 오늘도 내일도 모레도 먹히는 것을 원하지 않기에 위와 같이 state와 action의 sequence로 식을 바꿀 수 있다. 이 식은 강화학습 문제와 비슷하게 된다. cost function: 얼마나 나쁜 결과를 했는지를 나타낸다.(minimize) reward function: 얼마나 잘했는지를 나타낸다(maximize) cost function for imitation learning imitation learning에서 reward function은 $\bf r(s,a) = \log p(\bf a = \pi ^\star (s)|s)$($\bf p(\bf a = \pi ^\star (s)|s)$은 action과 optimal policy가 같을 확률)으로 나타내고 cost function은 위 식과 같이 $\bf c(s,a)$로 나타내고 이는 틀린 횟수로 볼 수 있다. Distribution mismatch analysis distribution mismatch 문제에서 time 축을 time step $\bf T$로 정의하고, cost function을 $\bf r(s,a) = \log p(a=\pi^\star (s) | s)$로 log likelyhood loss를 사용해도 되지만 간단하게 분석하기 위해서 zero-one loss를 사용한다. $$\bf c(s,a) = \begin{cases}0 &amp; \ \ \text{ if } \ a = \pi ^\star (s) \ \cr1 &amp; \ \ \ \text{otherwise.}\end{cases} - (8) $$ tightrope로 예시를 들면, 위 grid 그림처럼 나타낼 수 있다.(빨간색이 낭떠러지) 학습이 어느정도 되서 모든$\bf s \in \mathit{D_{train}}$에 대해 $\pi_{\theta}(a \ne \pi^\star (s) | s) \leq \epsilon$이라 가정하자. 학습데이터의 모든 state에서 사람과 다른 행동(줄에서 떨어짐)을 할 확률이 $\epsilon$ 보다 작다는 것을 말한다($\epsilon$을 mistake할 확률으로 볼 수 있고 worst case이다.). $\epsilon$은 training method에 따라 값이 달라지고 method가 좋을수록 값이 작아진다. 이때 total cost 기댓값의 upper bound는 다음과 같다. $$\bf \mathbb{E} \left[\ \mathsf{\underset{t}{\sum}}\ c(s_{t},a_{t})\right] \leq \epsilon T + (1-\epsilon)(\epsilon(T-1) + (1-\epsilon)(…))$$ 첫번째 step에서 mistake했을때 cost의 기댓값은 $\bf \epsilon \bf T$(mistake를 하면 그 이후 step에서는 계속 mistake하므로)이고 첫번째는 잘하고 두번째 step에서 mistake했을때 cost의 기댓값은 $\bf (1-\epsilon)\epsilon(T-1)$이고 $\bf T$ step만큼의 항이 나온다. 그리고 각가의 항이 $\bf O(\epsilon T)$(Order of T)이기 때문에 $\bf \mathbb{E} [\ \mathsf{\underset{t}{\Sigma}}\ c(s_{t},a_{t})]$의 bound는 $O(\epsilon T^2)$가 된다. 하지만 $O(\epsilon T^2)$는 좋지 않다. 왜냐하면 $\epsilon$이 아무리 작아도 $T$가 길어질수록 cost가 커지기 때문이다. 위에서 했던 가정 “모든 $\bf s \in \mathit{D_{train}}$”은 trained state만 고려하기에 적절한 가정은아니다(image classification에서 train과 test를 똑같은 사진을 쓰는 것과 같음, generalization 문제). 그래서 가정을 $\bf s \sim p_{train}(s)$으로 바꾸자. 먼저 DAgger를 적용했을 때 $p_{train}(s) \rightarrow p_\theta(s)$($p_{train}(s) = p_\theta(s)$)이 된다. 이때 total cost 기댓값의 upper bound는 $$\bf \mathbb{E} \left[\ \mathsf{\underset{t}{\sum}}\ c(s_{t},a_{t})\right] \leq \epsilon T$$이된다.(매 step마다 mistake할 확률이 $\epsilon$으로 동일하기 때문) 이제 DAgger를 쓰지않고 $\bf p_{train}(s) \ne p_{\theta}(s)$(train data의 상태분포와 trained된 상태분포가 다름)라 가정하자. $$ \bf p_{\theta}(s_{t}) = (1-\epsilon)^t p_{train}(s_{t}) + (1-(1-\epsilon)^t)p_{mistake}(s_t)$$ 여기서 $(1-\epsilon)^t$는 mistake를 하지 않을 확률이고 $\bf p_{mistake}(s_{t})$는 mistake했을때 확률분포이다. $$ \bf | p_{\theta}(s_{t}) - p_{train}(s_{t})| = \underset{s_{t}}{\sum} | p_{\theta}(s_{t}) -p_{train}(s_{t}) $$ total variation divergence between $p_\theta(s_t)$ and $p_{train}(s_t)$ $$\bf p_{\theta}(s_{t}) = (1-\epsilon)^t p_{train}(s_{t}) + (1-(1-\epsilon)^t)p_{mistake}(s_t)$$ $$\bf p_{train}(s_t) = (1-\epsilon)^t p_{train} + (1-(1-\epsilon)^t)p_{train}$$ 이므로 $\bf \left| p_{\theta}(s_{t}) - p_{train}(s_{t}) \right| = (1-(1-\epsilon)^t) \left| p_{mistake}(s_{t})-p_{train}(s_{t}) \right|$이된다. $$\bf \left| p_{\theta}(s_{t}) - p_{train}(s_{t}) \right| = (1-(1-\epsilon)^t) \left| p_{mistake}(s_{t})-p_{train}(s_{t}) \right| \leq 2(1-(1-\epsilon)^t) \leq 2\epsilon t$$ 두 확률분포의 차이의 절댓값($\bf \left| p_{mistake}(s_{t})-p_{train}(s_{t}) \right|$)의 최댓값은 2이고 $\bf -(1-\epsilon)^t \leq -(1-\epsilon t)$ for $\epsilon \in \left[0,1\right]$이므로 $\bf \left| p_{mistake}(s_{t})-p_{train}(s_{t}) \right|$의 upper bound는 $2\epsilon t$이 된다. cost의 기댓값은 아래와 같이 전개된다. $$\bf \underset{t}{\sum} \mathbb{E_{p_{\theta}(s_{t})}} \left[ c_{t} \right] = \underset{t}{\sum}\underset{s_{t}}{\sum} p_{\theta}(s_{t})c_{t}(s_{t})$$ $$\bf p_\theta c = p_{train}c + (p_\theta - p_{train})c \ \ \ \because \bf p_\theta = p_{train} + p_\theta - p_{train}$$ $$\bf \leq p_{train}c + |p_\theta-p_{train}|c \leq p_{train}c + |p_\theta-p_{train}|c_{max}$$ $$\bf p_\theta = p_{train} + p_\theta - p_{train}$$ $$\bf \underset{t}{\sum} \mathbb{E_{p_{\theta}(s_{t})}} \left[ c_{t} \right] = \underset{t}{\sum}\underset{s_{t}}{\sum} p_{\theta}(s_{t})c_{t}(s_{t}) \leq \underset{t}{\sum}\underset{s_{t}}{\sum} p_{train}(s_{t})c_{t}(s_{t}) + \left| p_{\theta}(s_{t}) -p_{train}(s_{t})\right|c_{max}$$ 여기서 $\bf c_{max}$(worst possible cost)는 cost definition에 의해 1이고 $p_{train}$의 cost 기댓값 $\bf \underset{s_{t}}{\sum}p_{train} (s_{t})c_{t}(s_{t})$은 $\epsilon$이기때문에 아래와 같이 전개된다. $$\bf \underset{t}{\sum} \mathbb{E_{p_{\theta}(s_{t})}} \left[ c_{t} \right] \leq \underset{t}{\sum}\epsilon + 2 \epsilon t \leq \epsilon T + 2 \epsilon T^{2}$$ 결국 $\bf O(\epsilon T^{2})$이 된다.]]></content>
      <categories>
        <category>lecture</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRML 1 Introduction]]></title>
    <url>%2F2019%2F08%2F31%2F2019-08-31-PRML-1%2F</url>
    <content type="text"><![CDATA[0.Introduction패턴인식(Pattern Recognition) &amp; 머신러닝(Machine Learning) Pattern Recognition이란 computer 알고리즘을 사용하여 데이터에 내재된 패턴을 자동으로 찾는 것을 의미한다. 아래와 그림과 같이 필기체를 분류(classification)하는 문제가 있다고 하자 $\bf x$:숫자의 이미지 $\bf y$: 숫자의 class 다음과 같은 문제는 Rule-based(heuristics)한 방법으로 해결할 수 있지만 실제로 이러한 방법은 예외사항을 계속해서 만들어야 하므로 안 좋은 결과를 만들게 된다. 더 좋은 방법은 training set을 사용하여 model의 parameters를 조절하는 machine learning을 사용하는 것이다. machine learning의 결과는 $\bf x$를 input으로 받고 output으로 $\bf y$를 내보내는 $y({\bf x})$로 표현할 수 있다. 이 함수는 training data를 기반으로 training(learing)을 거치며 원하는 함수에 가까워 진다. training된 모델로 test set 이라 부르는 새로운 데이터의 결과를 알 수 있습니다. 일반화(Generalization) training 동안 사용되지 않은 data를 올바르게 분류하는 능력을 generalization이라 한다. generalization은 pattern recognition의 주된 목적이다. 전처리(Pre-Process) 대부분의 상황에서 입력데이터를 문제를 더 쉽게 해결하기 위해서 전처리(pre-process)라 한다. 이러한 전처리과정은 test data도 train data에서 한 것과 똑같이 적용해야 한다. 전처리 과정은 data의 목적은 연산 속도를 높이고 데이터를 더 쉽게 다루기 위해서 이다. 전처리 과정에서 정보를 잃게되는데 그로인해 전체적인 성능이 저하되지 않도록 조심해야한다. Machine Learning의 종류지도학습(Supervised Learning) Problem input vectors와 상응하는 target vectors가 있는 문제 분류(classification) target vectors가 유한개의 discrete categories인 문제 예) 필기체 숫자 분류 회귀(regression) target vectors가 continuous 변수인 문제 예) 주식 가격 예측 비지도학습(Unsupervised Learning) Problem target vectors없이 input vectors만 존재하는 문제 클러스터링(clustering) 데이터내에서 비슷한 속성을 가지는 그룹을 찾는 문제 밀도 추청(density estimation) 입력 데이터로 변수의 분포를 도출하는 문제 (변수 $\neq$ 입력데이터) 시각화(visualization) 고차원 공간의 데이터를 2 또는 3 차원 공간에 투영하는 보여주는 것 강화학습(reinforcement learning) 주어진 상황에서 reward를 최대화 하기위한 적절한 actions를 찾는 문제 강화학습은 지도학습처럼 optimal한 outputs이 주어지지 않고 trial and error를 통해서 학습한다. 대부분의 경우, 현재의 action은 즉각적인 reward 뿐만 아니라 이후의 reward에도 영향을 끼칩니다. 어떤 action은 좋은 결과를 어떤 action은 덜 좋은 결과를 불러오는데 이러한 것을 정확히 알지 못하는 문제를 신뢰 활당 문제(credit assignment problem)이라 한다. 강화학습의 특징 중 하나는 탐색(exploration, 효율적인 새로운 행동을 찾는 것)과 활용(exploitation, 높은 보상을 주는 쪽으로 행동하는 것)의 trade-off 문제를 해결하는 것이다. 이 책에서는 다루지 않는다. 1. Example: Polynomial Curve FittingPolynomial Curve Fitting 0과 1사이에서 10개의 샘플을 뽑아 입력 변수를 $x$라고 하자. 초록색 선은 $\sin (2\pi x)$ 이다. 우리의 목표는 새로운 입렵 변수의 타겟 값 $t$를 올바르게 예측하는 것이다. 위 그림에서 주어진 데이터들은 랜덤하게 noise가 더해져 있다.(푸른색 동그라미가 초록색선위에 있지 않다.) 이 문제를 간단한 curve fitting 방식으로 접근해보자 $$ y(x,{\bf w})=w_0+w_1x+w_2x^2+…+w_Mx^M=\sum_{j=0}^{M}w_jx^{j} \qquad{(1.1)} $$ $M$: 다항식의 차수 $y(x,{\bf w})$가 $x$에 대하여 비선형이지만 계수 ${\bf w}$에 대해서는 선형 함수이다. 다항 함수와 같이 알려지지 않는 변수 ${\bf w}$에 대해 선형인 함수를 선형 모델(linear models)라 불린다. (Chapters 3와 4에 자세히 다룸) error function 계수 ${\bf w}$는 training data를 통해 다항 함수를 fitting함으로써 결정된다. 이러한 방법은 error function(함수 $y(x,{\bf w})$와 target 변수$t$의 차이)를 최소화 함으로써 가능하다. 간단한 error function의 예로 오차의 제곱합이 있다. $$ E({\bf w})=\dfrac{1}{2}\sum_{n=1}^{N}{y(x_n,{\bf w})-t_n}^2 \qquad{(1.2)} $$ $\frac{1}{2}$는 계산의 편의를 위해서 사용됨 주어진 error function이 이차형식(quadratic)의 함수 꼴이므로 최소화 하는 값은 유일 해를 가지게 됨을 보장 받는다. 결과적으로 얻어지는 함수 값은 $y(x, {\bf w}^*)$이다. ${\bf w}^*$는 유일 해를 가질때 ${\bf w}$이다. Model Comparison(Model Selection) 다항식의 차수 $M$를 고르는 문제가 있는데 이를 model comparison 또는 model selection이라 한다. $M=0$,$M=1$일때 training data와 목표인 $\sin (2\pi x)$모두 fit하지 않는데 이런 경우를 under-fitting이라 한다. $M = 9$일때 training data와 가장 잘 fit하지만 $\sin (2\pi x)$와는 fit 하지 않는 것을 확인할 수 있다. 이런 경우를 over-fitting이라 한다. 우리의 목표는 새로운 데이터에 대해서 정확한 예측하는 좋은 일반화(generalization) 모델을 찾는 것이다.]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[공업 수학 7강]]></title>
    <url>%2F2019%2F08%2F15%2F2019-08-15-engineering-mathematics%2F</url>
    <content type="text"><![CDATA[7.1 행렬, 벡터: 합과 스칼라곱행렬(matrix) 수(혹은 함수)를 직사각형 모양으로 괄호 안에 배열한 것(사각행렬) 원소(Entry) 또는 요소(Element): 행렬에 배열되는 수(혹은 함수) 행(Row) : 수평선 열(Column) : 수직선 일반적인 표기법과 개념$$ {\bf A} = [a_{jk}] =\begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \cra_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \cr\vdots &amp; \vdots &amp; \ddots &amp; \vdots \cra_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\end{bmatrix} : m \times n 행렬 $$ 행렬은 볼드체의 대문자를 사용한다. 첫 번째 아래 첨자 $j$는 행(Row) 두 번째 아래 첨자 $k$는 열(Column) $a_{jk}$: $j$ 행, $k$ 열의 원소(Element) $m \times n$은 행렬의 크기(size)를 나태는 것이다. 정방행렬(square matrix) $m=n$ 이라면 A는 정사각형 모양이다 정방행렬에서 원소 $a_{11}, a_{22}, \cdots , a_{nn}$을 포함하는 대각선을 행렬 $A$ 의 주대각선(Principal Diagonal)이라고 한다 $ {\bf A} = [a_{jk}] =\begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \cra_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \cr\cdot &amp; \cdot &amp; \cdots &amp; \cdot \cra_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}\end{bmatrix}$ 계수행렬(coefficient matrix) 선형연립방정식의 계수 만으로 이루어진 행렬 *통상, 선형 연립방정식을 $\bf Ax = b$ 로 나타낼 때, 행렬 A를 말한다.ex) $ \bf \widetilde A =\begin{bmatrix}4 &amp; 6 &amp; 9 \cr6 &amp; 0 &amp; -2 \cr5 &amp; -8 &amp; 1\end{bmatrix}$ 첨가행렬(augmented matrix) 계수행렬 및 우변 상수항을 모두 포함한 행렬 계수행렬 및 우변 상수항을 모두 포함한 행렬 각 행이 선형연립방정식의 하나의 식과 대응되는 행렬 ex) $ \bf \widetilde A =\begin{bmatrix}4 &amp; 6 &amp; 9 &amp; 6 \cr6 &amp; 0 &amp; -2 &amp; 20 \cr5 &amp; -8 &amp; 1 &amp; 10\end{bmatrix}$ 벡터(Vector) 단 하나의 행, 또는 하나의 열만으로 이루어진 행렬 벡터의 성분을 성분(component)라 한다. 벡터는 소문자 볼드체 문자 $\bf a, b, c,\cdots,$ 또는 $\bf a = [a_j]$로 표현한다. 행벡터(Row Vector) 하나의 행으로 구성된 행렬 $$ {\bf a} = \begin{bmatrix} a_1, a_2, \cdots, a_n \end{bmatrix}$$ 열벡터(Column Vector) 하나의 열로 구성된 행렬 $$ {\bf b} = \begin{bmatrix} b_1\cr b_2\cr \vdots \cr b_n \end{bmatrix} $$ 행렬의 상등(Equality of Matrices) 두 행렬 A와 B의 크기가 같고 대응하는 성분들이 모두 같을 경우 ($A = B$) 행렬의 합(Matrix Addition) 같은 크기의 행렬에 대해서만 정의되고 그 합은 대응하는 원소를 각각 합함으로 얻어진다. 스칼라곱(Scalar Multiplication) 행렬의 각 원소에 상수(Scalar)를 곱(Product)하여 얻어진다. 행렬의 가법과 스칼라곱에 대한 연산법칙 7.2 행렬의 곱행렬과 행렬의 곲(Matrix Multiplication) $r \times p$행렬 $B = [b_{jk}]$의 행수 $r$와 $m \times n$행렬 $A = [a_{jk}]$의 열수 $n$가 서로 같아야 정의되며 $$c_{jk} = \sum_{j=1}^n a_{jl}b_{lk} = a_{j1}b_{1k} +a_{j2}b_{2k} + \cdots + a_{jn}b_{nk}$$를 원소로하는 $m\times p$ 행렬로 정의된다.($AB$는 정의되지만 BA는 정의되지 않을 수 있다.) 행렬의 곱은 비가환적(Not Commutative)이다. 즉 $AB \neq BA$ 행렬의 곱에 대한 연산 법칙$$ (kA)B = k(AB) = A(kB)$$$$ A(BC) = (AB)C\ \ \ \ \ \text{(결합법칙(Associative Law))}$$$$ (A+B)C = AC + BC\ \ \text{(분배법칙(Distrivutive Law))}$$$$ C(A+B) = CA + CB\ \ \text{(분배법칙(Distrivutive Law)} $$ 행렬과 벡터의 전치(Transposition of Matrices) 열과 행이 서로 바뀌어 얻어진 행렬 $$ {\bf A^T} = [a_{kj}] =\begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{m1} \cra_{21} &amp; a_{22} &amp; \cdots &amp; a_{m2} \cr\vdots &amp; \vdots &amp; \ddots &amp; \vdots \cra_{1n} &amp; a_{2n} &amp; \cdots &amp; a_{mn}\end{bmatrix}$$ 정방행렬에 대한 전치는 주대각선에 관하여 대칭으로 위치된 원소들을 서로 바꾼 것이다. 전치 연산에 대한 법칙$$ (A^T)^T =A $$$$ (A + B)^T = A^T +B^T $$$$ (cA)^T = cA^T $$$$ (AB)^T = B^TA^T $$ 특수한 행렬(Special Matrices)대칭행렬(Symmetric Matrix) 전치가 본래의 행렬과 같은 정방행렬 ($A^T = A$) 반대칭행렬(Skew- symmetric Matrix) 전치가 본래의 행렬의 음이 되는 정방행렬 ($A^T = -A$) 삼각행렬(Triangular Matrix) 위삼각행렬(Upper Triangular Matrix) 주대각선을 포함하여 그 위쪽으로만 0이 아닌 원소를 갖는 정방행렬 아래삼각행렬(Lower Triangular Matrix) 주대각선을 포함하여 그 아래쪽으로만 0이 아닌 원소를 갖는 정방행렬 대각행렬(Diagonal Matrix) 주대각선 상에서만 0이 아닌 원소를 가질 수 있는 정방행렬 스칼라 행렬(Scalar Matrix) 주대각선 원소들이 모두 같은 대각행렬 단위행렬(Unit 또는 Identity Matrix) 주대각선 원소들이 모두 1은 대각행렬 7.3 선형연립방정식, Gauss 소거법선형연립방정식(=선형계(linear systems)) 제차연립방정식(Homogeneous Simultaneous System) $b_j$가 모두 0인 경우 비제차연립방정식(Nonhomogeneous Simultaneous System) $b_j$중 적어도 하나는 0이 아닌 경우 선형연립방정식의 행렬표현 : $\bf Ax = b$ 계수행렬(Coefficient Matrix) : $\bf A$ 해벡터(Solution Vector) :$\bf x$ 첨가행렬(Augmented matrix) : 계수행렬 $\bf A$에 열벡터 $\bf b$를 첨가한 행렬 가우스 소거법과 후치환(Gauss Elimination and Back Substitution) $x_1$을 소거 : 첫 번째 식에 두 배 한 후, 이를 두 번째 식에 더한다. 후치환(Back Substitution) :$x_2, x_1$ 순으로 해를 구한다. 마지막 방정식에서 해를 구한 후, 그 결과를 역순으로 첫째 방정식에 대입하여 정리한다. 기본행연산. 행동치 연립방정식(Elementary Row Operations. Row-Equivalent Systems) 기본 행연산을 이용하여 미지수를 하나씩 소거하여 대각선 아래의 계수를 0으로 만든다 행동치(Row-Equivalent) 선형시스템 $S_1$이 선형시스템 $S_2$에 유한번의 기본행연산을 가하여 얻어질 수 있다면 $S_1$을 $S_2$의 행돋치라 한다. 행동치 연립방정식(Row-Equivalent Systems) 행동치 연립방정식들은 같은 해집합을 갖는다. 지금까지 행연산에 국하여 다루었고 열연산의 경우 해집합에 영향을 미칠 수 있다. 미지수보다 더 많은 방정식을 가진다면 과잉한정(overdetermined) 미지수보다 적은 방정식을 가지면 과소한정(underdetermined) 행사다리꼴(Row Echelon Form)과 행 사다리꼴로부터의 정보행사다리꼴 Gauss 소거법의 마지막 단계에서 보는 계수행렬과 첨가행렬의 형태와 이에 대응하는 연립방정식각 행에서 가장 왼편에 있는 0아닌 성분을 모두 1로 만든 것을 축소된 사다리꼴(reduced echelon form)이라 한다. 3가지 가능한 경우 정확하게 하나의 해가 존재한다. : $r=n$이고 $\widetilde{b_{r+1}}, \cdots, \widetilde{b_m}$이 모두 0이다. 무한히 많은 해가 존재한다. : $r &lt; n$이고 $\widetilde{b_{r+1}}, \cdots, \widetilde{b_m}$이 모두 0이다. 해가 없다. : $r &lt; m$이고 $\widetilde{b_{r+1}}, \cdots, \widetilde{b_m}$중 하나라도 0이 아니다.(모순이 없는 경우, $r = m$이거나$, $r &lt; m$이더라도 $\widetilde{b_{r+1}}, \cdots, \widetilde{b_m}$가 모두 0이라면 해가 존재한다.) 7.4 일차 독립. 행렬의 계수. 벡터공간벡터의 일차 독립과 종속성 일차 독립(Linearly Independent) : 모든 $c_j = 0$ 일 때만 위 식이 만족 일차 종속(Linearly Dependent) : 어떤 $c_j \neq 0$ 이어도 위 식이 만족 행렬의 계수(Rank) 행렬에서 1차독립인 행벡터의 최대수이며 rank($\bf A$)라 표시 행동치인 행렬 행동치인 행렬들은 같은 계수를 갖는다. 일차종속성과 일차독립성 각각 $\bf n$개의 성분을 갖는 $\bf p$개의 벡터들은 이 벡터들을 행벡터로 취하여 구성된 행렬의 계수가 $\bf p$이면 일차독립이고, 그 계수가 $\bf p$보다 작으면 일차종속이다. 열벡터에 의한 계수 행렬의 계수는 행렬의 일차독립인 열벡터의 최대수와 같다.=&gt; 행렬과 행렬의 전치는 같은 계수를 갖는다 벡터의 일차종속 $\bf n$개의 성분을 갖는 벡터가 $\bf p$개 있고 $\bf n &lt; p$ 라면 이들 벡터들은 항상 일차종속이다. 벡터공간 (Vector Space) 공집합이 아닌 벡터의 집합에 속해 있는 임의의 두 원소에 대하여, 이들의 일차결합이 다시 집합의 원소가 되며 다음 법칙을 만족하는 벡터들의 집합 차원(Dimension) 벡터공간내의 일차독립인 벡터들의 최대수이며 $\bf \dim(V)$로 표기 기저(Basis) 벡터공간내의 최대로 가능한 수의 일차독립인 벡터로 구성되는 부분집합이며 기저가 되는 벡터의 수는 차원과 같다. 생성공간(Span) 성분의 수가 같은 벡터들에 관한 일차결합으로 표환되는 모든 벡터들의 집합 부분공간(Subspace 벡터공간에서 정의된 벡터합과 스칼라곱에 관하여 닫혀있는 부분집합 $\bf R^n$ 벡터공간 $\bf n$개의 성분을 갖는 모든 벡터들로 이루어진 벡터공간 $\bf R^n$의 차원 $n$이다. 행공간(Row Space) 행벡터들의 생성공간 열공간(Column Space) 열벡터들의 생성공간 행공간과 열공간 행렬의 행공간과 열공간은 차원이 같고, 행렬의 계수와도 동일하다. 영공간(Null Space) $\bf Ax = 0$의 해집합 퇴화차수(Nullity) 영공간의 차원 $\bf A$의 계수 + $\bf A$의 퇴화차수 = 행렬 $\bf A$의 행 갯수 7.5 선형연립방정식의 해 : 존재성, 유일성선형연립방정식에 대한 기본정리존재성(Existence) 선형연립방정식이 모순이 없기 위한(Consistent), 다시 말해서 해를 갖기 위한, 필요충분조건은 계수행렬과 첨가행렬이 같은 계수를 갖는 것이다. 유일성(Uniqueness) 선형연립방정식이 유일한 해를 갖기 위한 필요충분조건은 계수행렬과 첨가행렬이 같은 계수를 갖는 것이다 무수히 많은 해(Infinitely Many Solutions) 계수행렬과 첨가행렬이 같은 계수$\bf r$을 갖고 $\bf r &lt; n$이면, 무수히 많은 해가 존재한다. Gauss 소거법(Gauss Elimination) 해가 존재하면 Gauss 소거법에 의해 모두 구해질 수 있다. 제차연립방정식 제차연립방정식은 항상 자명한 해(Trivial Solution)을 갖는다. 자명하지 않은 해가 존재할 필요충분조건 : $\bf r &lt; n$ (계수행렬의 계수= $\bf r = \text{rank} A$, 미지수의 갯수 = $ \bf n$) $\bf r &lt; n$이면 해공간은 $\bf n-r$차원 벡터공간이다. 제차연립방정식의 두 해벡터의 일차결합도 제차연립방정식의 해이다. 미지수보다 방정식의 수가 적은 제차 선형연립방정식 방정식의 수가 미지수의 수보다 적은 제차연립방정식은 항상 자명하지 않은 해(Nontrivial Solution)를 갖는다. 비제차연립방정식 만약 비제차 연립방정식이 해를 갖는다면 모든 해는$$\bf x = x_0 + x_h$$와 같은 형태가 된다.$\bf x_0$은 고정된 임의의 해이고 $\bf x_h$는 대응하는 제차연립방정식의 모든 해를 대표한다. 7.6 참고사항 : 2차 및 3차 행렬식2차 행렬식(Determinant of Second Order) 선형연립방정식 3차 행렬식(Determinant of Third Order) 선형연립방정식 7.7 행렬식. Cramer의 법칙$\bf n$차 행렬식(Determinant of Third Order) 소행렬식(Minor): $M_{jk}$ 여인수(Cofactor): $C_{jk}$ 기본행연산항(Elementary Row Operation)에서의 $\bf n$차 행렬식의 변화 (a) 두 행을 바꾸는 것은 행렬식의 값에 -1을 곱하는 것이다 (b) 한 행의 상수배를 다른 행에 더하는 것은 행렬식의 값에 변화를 주지 않는다. (c) 한 행에 0이 아닌 $c$를 곱하면 행렬식의 값이 $c$배가 된다. $\bf n$차 행렬식의 추가적인 성질 (a)-(c)는 열에 대해서도 성립한다. (d) 전치(Transposition)는 행렬식의 값에 변화를 주지 않는다. (e) 0행 또는 0열은 행렬식의 값을 0으로 만든다. (f) 두 행이나 두 열이 비례관계에 있으면 행렬식의 값은 0이다. 특히 같은 두 행이나 두 열을 가진 행렬식의 값은 0이다. 행렬식과 계수 $m \times n$ 행렬 $A=[a_{jk}]$가 계수 $r(\geq 1)$을 갖기 위한 필요충분조건은(1) $\bf A$가, 0이 아닌 행렬식을 갖는 $r \times r$ 부분행렬을 가지고,(2) $\bf A$의 $(r+1) \times (r+1)$ 또는 그보다 큰 크기의 모든 정방 부분행렬(그런 부분행렬이 존재할 경우)의 행렬식이 0이 되는 것이다.특히, $\bf A$가 정방행렬 $ n \times n$ 정방행렬일 때, 계수가 $n$일 필요충분조건은(3) $\text{det}A \neq 0$이다. Cramer의 정리(행렬식에 의한 선형연립방정식의 해) $D \neq 0이면 그 연립방정식은 오직 하나의 해를 갖는다. $D_k$는 $D$의 $k$번째열을 $b_1, \cdots, b_n$을 성분으로 하는 열벡터로 대치하여 얻은 행렬식이다. 따라서 위 식이 제차이고 $D \neq 0$이면, 그것은 오직 자명한 해 $x_1 = 0, x_2 = 0, \cdots, x_n = 0$만을 갖는다. 만약 $D = 0$이면, 제차연립방정식은 자명한 해가 아닌 해도 갖는다. 7.8 역행렬. Gauss-Jordan 소거법역행렬(Inverse Matrix) 정칙행렬(Nonsingular Matrix) : 역행렬을 갖는 경우 특이행렬(Singular Matrix) : 역행렬을 갖지 않는 경우 역행렬을 가지면 그 역행렬은 유일하다 역행렬의 존재성 $\bf A$가 $\bf n \times n$행렬일 때, 역행렬$A^{-1}$이 존재할 필요 충분조건은 $\text{rank}A = n$이다.($\bf \text{det}A \neq 0$도 같은 조건이다) Gauss-Jordan 소거법에 의한 역행렬의 결정 행렬식에 의한 역행렬 공식 여인수 $C_{jk}$가 놓인 위치는, 행렬 $\bf A$의 성분 $a_{jk}$가 놓인 자리가 아니라, 성분 $a_{kj}$가 있는 자리이다. 대각행렬의 역행렬 대각행렬 ${\bf A} =[a_{jk}](즉, $j \neq k$일 때 $a_{jk} = 0$)가 역행렬을 가질 필요충분조건은 주대각선 상의 모든 성분 $a_{jj}$가 0이 아니어야 한다. $$ 이 경우 \bf A^{-1}은 \frac 1 a_{11}, \cdots, \frac 1 a_{nn}들이 대각원소인 대각행렬이 된다.$$ 두 행렬의 곱 $\bf AC$의 역행렬$$\bf(AC)^{-1} = C^{-1}A^{-1}$$$$\bf (AC\cdotsPQ)^{-1} = Q^{-1}P^{-1}\cdots C^{-1}A^{-1}$$ 역행렬의 역행렬$$\bf (A^{-1})^{-1} = A$$ 행렬의 곱에 대한 특이 성질, 소거법 행렬의 곱은 교환법칙이 성립하지 않는다.(일반적으로 성립하지 않는다.) $$ \bf AB \neq BA$$ $\bf AB = 0$일 때 $\bf A = 0$ 또는 $\bf B = 0$이 아닐 수도 있다. $$\begin{bmatrix}1 &amp; 1 \cr2 &amp; 2\end{bmatrix}\begin{bmatrix}-1 &amp; 1 \cr1 &amp; -1\end{bmatrix} =\begin{bmatrix}0 &amp; 0 \cr0 &amp; 0\end{bmatrix}$$ $\bf AC = AD$일 떄 $C \neq D$일 수도 있다.(심지어 $A \neq 0$ 일 때에도)) 소거법칙 $\bf A,B,C$를 $n \times n$ 행렬이라 하자. (a) $\text{rank} A = n$이고 $\bf AB = AC$이면, $B = C$이다. (b) $\text{rank} A = n$이면 $\bf AB = 0$은 $\bf B = 0$을 의미한다. 그러므로 $\bf AB = 0$이면서 $\bf A \neq 0$이고 동시에 $\bf B \neq 0$이면, $\text{rank} {\bf A} &lt; n$이고 $\text{rank} {\bf B} &lt; n$이다. $\bf A$가 특이행렬이면, $\bf BA$와 $\bf AB$도 특이행렬이다. 행렬곱의 행렬식 $\bf \det (AB) = \det (BA) = \det A \det B 7.9 벡터공간, 내적공간, 일차변환실벡터공간(Real Vector Space) 성분 $\bf a,b,\cdots$을 갖는 공집합이 아닌 집합 $V$에 대하여 “벡터의 덧셈”과 “스칼라 곱”이라고 하는 두 가지 대수학적 연산법칙이 다음과 같이 정의되어 있으면, 이 집합 $V$를 실벡터공간(real vector space, 또는 실선형 공간(real linear space))이라 부르고 $V$의 성분을 벡터라 부른다. 벡터의 덧셈: $\bf a + b$ 가환성(Commutativity) $\bf a+b = b+a$ 결합성(Associativity) $\bf (u+v) + w = u + (v+w)$ 영벡터(Zero Vector) $\bf a+0=a, a+(-a)=0$ 스칼라곱: $k{\bf a}$ 분배성(Distributivity) $c({\bf a+b}) = c{\bf a} + c{\bf b}$ 분배성(Distributivity) $(c+k){\bf a} = c{\bf a} + k{\bf a}$ 결합성(Associativity) $c(k{\bf a}) = (ck){\bf a}, 1{\bf a} = {\bf a}$ 실내적공간(Real Inner Product Space) 실벡터공간 $V$에 속한 임의의 한 쌍의 벡터 $\bf a,b$에 대하여 하나의 실수를 대응시키는 규칙이 존재하여 다음 공리를 만족한다면, $V$를 실내적공간(real inner product space)이라 부른다. 내적(Inner Product)$$ (a,b) = a \cdot b $$ 선형선 $q_1{\bf a} + q_2{\bf b,c} = q_1({\bf a,c}) + q_2({\bf b,c})$ 대칭성 $\bf (a,b) = (b,a)$ 양의정치성(Positive-definiteness) $\begin{cases}(a,a) \geq 0, \cr(a,a) = 0 \text{일 필요충분조건은} \ \ a = 0\end{cases}$ 직교(Orthogonal) 내적이 영인 두 벡터 벡터의 길이 또는 노름(norm)$$\bf \lVert a \rVert = \sqrt{(a,a)} (\geq 0)$$ $n$차원 Euclid 공간$$\bf (a,b) = a^Tb = a_1b_1 + \cdots + a_nb_n$$ 위 식과 같이 약속된 내적이 정의되었다고 하면, 이 때 이 공간을 $n$차원 Euclid 공간이라 부르고 $E^n$(또는 $R^n$)이라 표기한다. Euclid 노름(Euclidean norm) 단위벡터(Unit Vector) 길이가 1인 벡터 Cauchy-Schwarz 부등식$$\bf \lvert (a,b) \rVert \geq \lVert a \rVert \lVert b \rVert $$ 삼각부등식$$\bf \lVert a+b \rVert \geq \lVert a \rVert + \lVert b \rVert $$ 평행사변형 등식$$\bf {\lVert a+b \rVert}^2 + {\lVert a-b \rVert}^2 = 2({\lVert a \rVert}^2 + {\lVert b \rVert}^2) $$ 일차변환(Linear Transformations) $\bf X$에서 $\bf Y$로의 사상(mapping) 또는 변환(transformation), 연산자(operator) 공간 $\bf X$의 벡터 $\bf x$에 대하여 공간 $\bf Y$의 유일한 벡터 $\bf y$를 대응(이와 같은 사상을 $\bf F$와 같은 대문자로 표기하자) 공간 $\bf X$의 벡터 $\bf x$에 대응하는 $\bf Y$의 벡터 $\bf y$를 $\bf F$에 의한 $\bf x$의 상(image)이라 하고 $\bf F(x)(또는 괄호 없는 Fx)$로 표시한다. $\bf F$를 선형사상(linear mapping) 또는 일차변환(linear transformation) $\bf X$의 임의의 벡터 $\bf v, x$와 임의의 스칼라 $c$에 대하여 다음의 식을 만족$$\bf F(v + x) = F(v) + F(x)$$$${\bf F}(c {\bf x}) = c{\bf F}({\bf x}) $$ $R^b$ 공간에서 $R^m$ 공간으로서의 선형변환 $\bf X = R^n$, $\bf Y = R^m$이라 하자 $m \times n$ 행렬 $\bf A$가 주어지면 $R^n$에서 $R^m$으로 의 변환$$\bf y = Ax $$$\bf A(u + x) = Au + Ax$와 ${bf A}(c{bf x}) = c\bf{ A x}$이므로 이 변환은 선형이다.$\bf F$는, $\bf R^n$과 $\bf R^m$ 공간에 각각 주어진 기저를 이용하여, 적절한 $m \times n$행렬 $\bf A$로 나타낼 수 있다. http://contents2.kocw.or.kr/KOCW/document/2016/hanbat/kimdongsoo/7.pdf https://latexbase.com/d/e2c4eeb2-68f6-4efa-a305-112097ad9e8b https://forknwork.wordpress.com/2018/02/14/openpose3d-pose-baseline/ https://github.com/wangzheallen/awesome-human-pose-estimation#3d-pose-estimation]]></content>
      <categories>
        <category>mathematics</category>
      </categories>
      <tags>
        <tag>linear algebra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CVPR 2017] Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields 리뷰]]></title>
    <url>%2F2019%2F08%2F02%2F2019-08-02-Openpose-paper-review%2F</url>
    <content type="text"><![CDATA[Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields을 읽고 정리한 글입니다. IntroductionHuman pose estimation 문제는 다음과 같은 어려움 점이 존재합니다. Challenges Unknown number of people that can occur in a frame. (한프레임에 사람에 몇명이 있는지 모른다.) Complex Spatial Interference - Contact, Occlusion between people. (사람들간의 접촉, 맞물림등 복잡한 공간적 간섭이 존재) Variance in person scales (사람들의 크기가 다양함) Run time Complexity. (실행시간이 길다) Human pose estimation 문제를 접근하는 방법을 크게 Top-down Approach, Bottom-up Approach 두가지 방법이 있습니다. Top-down Approach Fig1. Top-down Appraoch방식은 이미지에서 사람을 먼저 찾고 관절을 추정하는 방식입니다. Problem Human detector가 사람을 잘못찾으면 이를 해결할 방법이 없다. 사람의 수가 많아지면 computational cost가 증가한다. ( Challenges 4 ) Bottom-up Approach Fig2. Bottom-up Appraoch방식은 관절을 먼저 다찾고 이를 알맞게 이어주는 방식입니다. Problem 찾은 관절을 매칭할 수 있는 조합이 매우 많고 이를 적절하게 매칭하는데 시간이 많이 걸리고 Accuracy도 높이는 것이 힘듭니다. Architecture Fig3 Fig4 Input &amp; 2 BranchInput: RGB color image 병렬적으로 구성된 2개의 Branch Branch 1에서는 Confidnce maps를 찾아낸다.$$S^1 = \rho^1(F), S^t = \rho(F,S^(t-1),L^(t-1), \forall t \ge 2) - (1)$$ Branch 2에서는 PAF를 찾아낸다.$$S^1 = \rho^1(F), S^t = \rho(F,S^(t-1),L^(t-1), \forall t \ge 2) - (2)$$ $S^t$:$\ $ Stage $t$에서 만들어진 Confidence maps $F$:$\ $ VGG-19에서 추출된 Feature $L^t$:$\ $ Stage $t$에서 만들어진 PAF Fig5.stage가 지날수록 성능이 향상된다.(위에 주황색원: False negative, 빨간원: False positive이 해결되고 있음 Loss functiongroundtruths와 추정된 값의 $L_2$ loss를 사용하였다. $$f_{S}^t = \sum\limits_{j=1}^J\sum_{p}W(p) \dot \lVert S_{j}^t(p) - S_{j}^{\ast}(p)\rVert_{2}^2 - (3)$$ $$f_{L}^t = \sum\limits_{c=1}^C\sum_{p}W(p) \dot \lVert L_{c}^t(p) - L_{c}^{\ast}(p)\rVert_{2}^2 - (4)$$ $J$,$C$: Confidence map의 개수와 PAF의 개수 $f_{S}^t$: Stage $t$ 에서의 Confidence map loss $f_{L}^t$: Stage $t$ 에서의 PAF loss $p$: 이미지의 좌표 $W(p)$: binary mask, true-positive를 처벌하는것을 피하기위해서 사용됨, annotation이 없을때 $W(p) = 0$ Stage 마다 loss를 계산하여 Vanishing gradient를 해결( paper) overall objective$$f = \sum\limits_{t=1}^T(f_{S}^t + f_{L}^t)- (5)$$ Generation of confidence mapdataset은 keypoint만 주어지기에 groundtruths에 될 confidence maps를 만들어 줘야한다. $$S_{j,k}^{\ast}(p) = \exp (-\dfrac{\lVert p-x_{j,k}\rVert _{2}^2}{\sigma^2}) - (6)$$ $S_{j,k}^{\ast}(p)$:$ \ \ k$번째 사람의 $j$번째 관절의 confidence map $x_{j,k}$:$\ \ k$번째 사람의 $j$번째 관절의 keypoint $\sigma$:$ \ $ peak의 범위를 조절함 Fig6. $$S_{j}^{\ast}(p) = \max_{\rm k}S_{j,k}^{\ast}(p) - (7)$$ 이렇게 만든 가우시안 분포에 max를 취해준다.(average보다 peaks가 distinct하게 남아서 근접에 대한 정밀도를 가진다.) PAFs(Part Affinity Fields) Fig7. (a):$ \ $ keypoint의 모든 연결 후보, (b):$ \ $ 각 연결 쌍의 중간점을 추가하여 중간 점의 발생률을 가지고 그리기(초록색선:틀린선,검은선:맞는선),사람들이 몰려있으면 오판될 수 있다.(이러한 방법은 위치만 고려하고 방향은 고려 하지 않고 region of support of limb를 한점으로 줄여버린다.) (c):$ \ $PAF를 이용한 결과(위치와 방향문제 해결 및 region of support of limb의 문제를 해결) Fig8. 식(4)를 풀기위해서 train때 PAF의 groundtruth를 다음과 같이 정의합니다.$$L_{c,k}^{\ast}(p) = \begin{cases}v &amp; \ \ \text{ if } \ p \ \text{on limb} \ c,k \cr0 &amp; \ \ \ \text{otherwise.}\end{cases} - (8) $$ $c$:$\ $limb의 종류 $v$:$\ \ v = \frac{(x_{j_2,k}-x_{j_1,k})}{\lVert x_{j_2,k}-x_{j_1,k}\rVert_2}$, limb의 방향의 unit vector(위 그림의 초록색선) limb위에 있는 points는 distance threshold내로 정의합니다. $$0 \le v \dot (p-x_{j_1,k}) \le l_{c,k} , \text{and} , |v_{\perp} \dot (p-x_{j_1,k})| \le \sigma_l - (9)$$ $l_{c,k}$: $\ \ l_{c,k} = \lVert x_{j_2,k} - x_{j_1,k}\rVert_2$, limb의 길이 $\sigma_l$:$ \ $ limb의 폭 마지막으로 이미지에 모든 사람으로 평균화 합니다. $$L_{c}^{\ast}(p) = \dfrac 1 n_{c}(p) \sum_{k}L_{c,k}^*(p) - (10)$$ $n_c(p)$:$\ $p에서의 non-zero vectors의 수(서로다른 사람의 limbs의 overlap되는 픽셀을 평균화시킴) test때 에는 $d_{j_1}$으로부터 $d_{j_2}$로 일정간격으로 선적분을 수행하여 affinity field의 세기(E)를 구합니다. $$E = \int_{u=0}^{u=1} L_c(p(u)) \cdot \dfrac {d_{j_2}-d_{j_1}} {\lVert d_{j_2}-d_{j_1} \rVert_2} du - (11)$$ $L_c$: $ \ $예측된 PAF $d_{j_1}$: $ \ $ 시작part 위치 $d_{j_2}$: $ \ $ 끝 part 위치 $p(u)$: $ \ $ $p(u) = (1-u)d_{j_1} + ud_{j_2}$, 2 part의 위치 사이를 채웁니다. Multi-Person parsing using PAFs Fig9 (a): Original image (b): 가능한 모든 연결 (c): 사람의 관절구조(spanning tree형태) (d): 이웃한 관절끼리 두개씩의 매칭 non-maximum suppression(NMS)을 사용해서 모든 part의 confidence maps를 구하였습니다. 이로인해 다양한 후보가 발생하고 여러 사람이 있기에 false positive 문제가 발생할 수 있습니다. optimal한 parse를 찾는 문제는 K-dimensional matching problem을 가지고있다.(NP-Hard 이라고도 함) 이 논문에서는 계속해서 high-quality한 match를 찾는 greedy relaxation을 말하고 있습니다.(optimal association 문제를 maximum weight bipartite graphmatching problem으로 줄입니다.) 어떠한 2개의 edge도 1개의 node를 공유하지 않기에 목표는 주어진 edges에서 maximum weight를 찾는 것입니다.(weight는 식(10)을 이용하여 구합니다) $$\max_{Z_c}E_c = \max_{Z_c} \sum_{m \in D_{j_1}}\sum_{n \in D_{j_2}}E_{mn}\dot z_{j_1j_2}^{mn} - (12)$$ $z_{j_1j_2}^{mn}$:$\ \ z_{j_1j_2}^{mn} \in \lbrace 0,1\rbrace$, detection candidates $d_{j_1]}^m$와 $d_{j_2}^n$이 연결되 있는지 아닌지를 나타내는 변수 $Z$:$\ \ Z = \lbrace z_{j_1j_2}^{mn} \text{for} j_1,j_2 \in \lbrace 1…J\rbrace,m\in \lbrace 1…N_{j_1}\rbrace,n \in \lbrace 1…N_{j_2}\rbrace\rbrace$, $z_{j_1j_2}^{mn}$ 가능한 모든 연결의 집합 $D_J$:$\ \ D_J = \lbrace d_j^m : \text{for} j \in \lbrace 1…J\rbrace,m\in \lbrace 1…N_j\rbrace\rbrace$,body part detection candidates의 집합 결론적으로 optimization은 다음과 같이 표현된다 $$\max_{Z} E = \sum_{c=1}^C\max_{Z_c}E_c - (13)$$ ( ? adjacent tree nodes는 PAFs에의해서 explicitly 하게 모델되었고 nonadjacent tree nodes는 CNN에의해 implicitly 모델되었습니다. ) ( ? 이러한 특성은 CNN이 large receptive field를 가지고 train 되고 non-adjacent tree nodes의 PAFs 또한 predicted PAF에 영향 끼쳤기에 발생하였습니다. ) ResultsMPII Multi-person Dataset COCO Dataset Referencehttps://ml.starall.kr/1 https://cloudup.com/i_gPL3kASQg]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>deep_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openpose 설치 tutorial]]></title>
    <url>%2F2019%2F04%2F08%2F2019-04-08-Openpose-tutorial%2F</url>
    <content type="text"><![CDATA[Openpose installrequirement NVIDIA CUDA_(그래픽카드 메모리가 1.6GB이상이여한다.)NVIDIA cuDNN Clone OpenPose$ git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose openpose/3rdparty 폴더에 가보면 이렇게 되어있을텐데 openpose github페이지에 가서 3rdparty에 들어간다. 여기서 caffe,pybind11에 들어가 깔아둔 openpose/3rdparty에 복사한다. 12345# openpose/3rdparty에서 터미널 실행하고$ git clone https://github.com/CMU-Perceptual-Computing-Lab/caffe.git$ git clone https://github.com/pybind/pybind11.git install library12345678910111213$ sudo apt-get install wget vim cmake cmake-qt-gui$ sudo apt-get install python-dev python-pip python-numpy$ pip install --upgrade pip$ sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev$ sudo apt-get install libhdf5-serial-dev protobuf-compiler$ sudo apt-get install libboost-all-dev libgoogle-glog-dev$ sudo apt-get install liblmdb-dev libopenblas-dev libatlas-base-dev cmake터미널 창에 cmake-gui 입력 다음과같이 where is the source code: Openpose 주소 where to build the binaries: openpose/build 로 바꿔준다. 그리고 configure를 눌러준다. 이런창이 나오면 generate 버튼을 클릭하고 완료되면 build 폴더로 간다. cd openpose/build/ 그리고 make 해준다 make -j 8 Demo 실행make가 완료되면 ./build/examples/openpose/openpose.bin --video examples/media/video.avi 을 시키고 잘작동하는지 확인한다 TODOtf버전 Openpsoe 추가]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep learning 유용한 사이트]]></title>
    <url>%2F2019%2F04%2F03%2F2019-04-04-Deep-learning-site%2F</url>
    <content type="text"><![CDATA[공부하면서 도움이 되었던 사이트들을 모아두는 곳 입니다. deep learning 기법Weight Initializationhttps://flonelin.wordpress.com/2018/01/28/weight-initalizer-%EC%A2%85%EB%A5%98/ https://gomguard.tistory.com/184 Regularizationhttp://www.hellot.net/new_hellot/magazine/magazine_read.html?code=202&amp;idx=41074&amp;public_date=2018-06 논문 분석http://openresearch.ai/ Pose estimationhttps://github.com/wangzheallen/awesome-human-pose-estimation#3d-pose-estimation 영상처리https://laonple.blog.me/220463627091 Codinghttps://github.com/Hvass-Labs/TensorFlow-Tutorials/ https://github.com/aymericdamien/TensorFlow-Examples https://github.com/tensorflow/docs/blob/master/site/en/tutorials/estimators/cnn.ipynb]]></content>
      <categories>
        <category>Information</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker,XML,JSON,CSV 간단 공부]]></title>
    <url>%2F2019%2F04%2F02%2F2019-04-02-docker-DATA%2F</url>
    <content type="text"><![CDATA[머신러닝, 딥러닝 실전 개발 입문 강의를 보고 공부한 내용입니다. https://www.youtube.com/watch?v=l_XFlB1Wwz8&amp;list=PLBXuLgInP-5m_vn9ycXHRl7hlsd1huqmS&amp;index=1 Docker먼저 docker 이미지를 가져온다(miniconda: anaconda 패키지중 가장 기본적인것만 설치되어있음) 1docker pull continuumio/miniconda3 설치가 다되었으면 아래 명령어로 이미지를 실행한다. 1docker run -i -t continuumio/miniconda3 /bin/bash |![alt](/images/Deep-learningetc/docker.png)| 위 이미지처럼 환경이 바뀌었으면 잘 작동한 것이다. 1exit 를통해서 환경을 종료 할 수 있다. 아래 명령어를 입력하면 1docker ps -a |![alt](/images/Deep-learningetc/docker (2).png)| 위 그림과 같이 컨테이너 실행 기록을 확인할 수 있다. 123docker commit 컨테이너 ID 이름:태그ex) docker commit &lt;a4997a3ede6e&gt; mlearn:init 위 명령어로 통해 컨테이너 이미지를 저장할 수 있다. 1docker run -i -t mlearn:init 위 명령어로 위에서 사용했던 환경과 똑같은 환경을 이용할 수 있다. 123docker run -i -t -v 자신이 가진폴더:컨테이너의 폴더 이미지 이름:태그 이름ex) docker run -i -t -v /home/kist-student/docker_sample:/sample mlearn:init 위 명령어로 폴더 마운트해서 이미지를 실행시킨다. XML(Extensible Markup Language)XML 형태여는 태그와 닫는 태그 &lt;태그&gt;&lt;/태그&gt; #요소(element) &lt;태그 /&gt; 콘텐츠&lt;태그&gt;콘텐츠&lt;/태그&gt;&lt;태그&gt; &lt;태그&gt;콘텐츠&lt;/태그&gt; &lt;태그&gt;콘텐츠&lt;/태그&gt;&lt;/태그&gt; 속성: “” =&gt; 문자열&lt;태그 속성=”값” 속성=”값” 속성=”값” 속성=”값”&gt;콘텐츠&lt;/태그&gt;&lt;태그 속성=”값” 속성=”값” 속성=”값” 속성=”값” /&gt; |![alt](/images/Deep-learningetc/XML.png)| Root tag 항상 하나,CDATA 내부 글자가 클때 데이터 보호용,rss는 태그이름 참고: https://sjh836.tistory.com/118 JSON(JavaScript Object Notation)JSON 구조가능한 자료형 숫자: 10, 253, 52.3 문자열: “안녕하세요” bool: true false null: null 배열:[10, 273, “안녕하세요”, true] 객체: 1234567&#123; &quot;키A&quot;: &quot;값&quot;, &quot;키B&quot;: 273, &quot;키C&quot;: true, &quot;키D&quot;: [12, 52] &quot;키E&quot;: &#123; &quot;name&quot;: 52 &#125;&#125; |![alt](https://www.w3resource.com/w3r_images/json-introduction.png)| 처음에는 배열이나 객체가 먼저오는게 일반적 CSV(Comma-Seperated Values)CSV 특징 한 줄에 데이터 하나 첫 번쨰 줄은 헤더로 사용 가능 1234ID, 이름, 가격1000,비누,300 # 1번 데이터1001,장갑,150 # 2번 데이터1002,마스크,230 # 3번 데이터 SSV: 뛰어쓰기TSV: tabCSV &gt; TSV, SSV xml 글자 많음(데이터 많음) &gt; json &gt; csv 표현력 많음: xml &gt; json &gt; csv xml은 잘 쓰이지 않고 있다고함 참고https://www.youtube.com/watch?v=dmwBi_JiYMs&amp;list=PLBXuLgInP-5m_vn9ycXHRl7hlsd1huqmS&amp;index=12 https://sjh836.tistory.com/118 https://stophyun.tistory.com/162 ##TODO데이터형 parsing 코드추가 및 docker 설명 추가]]></content>
      <categories>
        <category>theory</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNN Family 간단 정리]]></title>
    <url>%2F2019%2F04%2F01%2F2019-04-01-RCNN-Family%2F</url>
    <content type="text"><![CDATA[R-CNNRegion Proposals 입력 영상에서 ‘물체가 있을 법한’ 영역을 빠른 속도로 찾아내는 알고리즘을 region proposal 알고리즘이라 합니다. ex) Selective search, Edge boxes Transfer Learning기존의 만들어진 모델을 사용하여 새로운 모델을 만들시 학습을 빠르게 하며, 예측을 더 높이는 방법입니다. 사용되는 이유: 실질적으로 Convolution network을 처음부터 학습시키는 일은 많지 않습니다. 대부분의 문제는 이미 학습된 모델을 사용해서 문제를 해결할 수 있습니다. 복잡한 모델일수록 학습시키기 어렵습니다. 어떤 모델은 2주정도 걸릴수 있으며, 비싼 GPU 여러대를 사용하기도 합니다. layers의 갯수, activation, hyper parameters등등 고려해야 할 사항들이 많으며, 실질적으로 처음부터 학습시키려면 많은 시도가 필요합니다. 결론적으로 이미 잘 훈련된 모델이 있고, 특히 해당 모델과 유사한 문제를 해결시 transfer learining을 사용합니다. 새로 훈련할 데이터가 적지만 original 데이터와 유사할 경우데이터의 양이 적어 fine-tune (전체 모델에 대해서 backpropagation을 진행하는 것) 은 over-fitting의 위험이 있기에 하지 않습니다.새로 학습할 데이터는 original 데이터와 유사하기 때문에 이 경우 최종 linear classfier 레이어만 학습을 합니다. 새로 훈련할 데이터가 매우 많으며 original 데이터와 유사할 경우새로 학습할 데이터의 양이 많다는 것은 over-fitting의 위험이 낮다는 뜻이므로, 전체 레이어에 대해서 fine-tune을 합니다. 새로 훈련할 데이터가 적으며 original 데이터와 다른 경우데이터의 양이 적기 때문에 최종 단계의 linear classifier 레이어를 학습하는 것이 좋을 것입니다. 반면서 데이터가 서로 다르기 때문에 거의 마지막부분 (the top of the network)만 학습하는 것은 좋지 않습니다. 서로 상충이 되는데.. 이 경우에는 네트워크 초기 부분 어딘가 activation 이후에 특정 레이어를 학습시키는게 좋습니다. 새로 훈련할 데이터가 많지만 original 데이터와와 다른 경우데이터가 많기 때문에 아예 새로운 ConvNet을 만들수도 있지만, 실적적으로 transfer learning이 더 효율이 좋습니다. 전체 네트워크에 대해서 fine-tune을 해도 됩니다. https://fabj.tistory.com/57 R-CNN의 구조 이미지를 입력으로 받음 Selective search를 이용해 이미지로부터 약 2000개 가량의 region proposal을 추출함 각 region proposal 영역을 이미지로부터 잘라내고(cropping) 동일한 크기로 만든 후(warping), CNN을 활용해 feature 추출 각 region proposal feature에 대한 classification을 수행 ImageNet classification 데이터로 ConvNet을 pre-train 시켜 모델 $M$을 얻습니다. $M$을 기반으로, object detection 데이터로 ConvNet을 fine-tune 시킨 모델 $M^’$을 얻습니다. object detection 데이터 각각의 이미지에 존재하는 모든 region proposal들에 대해 모델 $M^’$으로 feature vector $F$를 추출하여 저장합니다. a. 추출된 $F$를 기반으로 classifier (SVM)을 학습합니다.b. 추출된 $F$를 기반으로 linear bounding-box regressor를 학습합니다. 여기서 CNN은 Transfer Learning을 사용 :모델의 마지막 층에 SVM을 두어 간단하게 이 결과가 객체인지 아니지, 객체가 맞다면 어떤 객체인지를 분류하도록 하였다. 문제점 localization에 취약함 -&gt; 개선: linear regression model을 통해 tight하게 맞추도록함 참고 모델이 Image feature를 생성하는 것(CNN), classifier가 class를 예측하는 것(SVM), regression model이 bouding box를 찾아낸 것 (linear regression) 총 3개가 필요합니다. R-CNN의 단점 Test 속도가 느림(CNN을 2000번 돌리기 때문) 학습과정이 복잡함(3단계 pipeline) Input image 크기를 강제로 224 x 224로 warp, crop Fast R-CNNSPP(Spatial Pyramid Pooling) 다양한 크기의 입력으로부터 일정한 크기의 feature를 추출해 낼 수 있는 방법 중 Bag-of-words (BoW)라는 방법이 있습니다. 하지만 BoW는 이미지가 지닌 특징들의 위치 정보를 모두 잃어버린다는 단점이 존재합니다. 이러한 단점을 보완하기 위한 Spatial Pyramid Pooling 은 이미지를 여러개의 일정 개수의 지역으로 나눈 뒤, 각 지역에 BoW를 적용하여 지역적인 정보를 어느정도 유지할 수 있게 됩니다. ROI Pooling(Single-level SPP) SPP layer는 feature map 상의 특정 영역에 대해 일정한 고정된 개수의 bin으로 영역을 나눈 뒤, 각 bin에 대해 max pooling 또는 average pooling을 취함으로써 고정된 길이의 feature vector를 가져올 수 있습니다. Fast R-CNN에서는 이러한 SPP layer의 single level pyramid만을 사용하며, 이를 RoI Pooling layer라고 명칭하였습니다. RoIPool의 핵심은 한 이미지의 subregion에 대한 forward pass값을 서로 공유하는 것이다. 위의 그림을 통해 어떻게 각 region에 대한 CNN feature가 feature map의 동일한 영역으로 부터 선택되어 값을 얻어내는지 확인할 수 있습니다. Fast R-CNN의 구조 pretrained된 모델에 이미지를 1개만 입력으로 받음 CNN을 통과한 feature map을 selective search로 2000개 가량의 region proposal을 추출함 region proposal(feature map)을 Roi pooling을 한다. Softmax classifier와 linear bounding-box regressor의 loss를 더하여 학습시킨다. 특징: CNN을 2000번 돌리는 것이 아닌 1번만 돌리면 된다.(속도 향상), 모델이 3개에서 1개의 네트워크로 통일됨 Fast R-CNN의 장단점장점 R-CNN에 비해 detection/localization 정확성 및 속도 개선 단점 region proposal 시간을 포함 시 real-time X (region proposal 에서 병목 현상) Faster R-CNNRPN(Region Proposal Network)Fast R-CNN 중 Selective Search(Region proposal)부분을 딥러닝으로 바꾼 것을 RPN이라 한다. 즉, 이 CNN기반의 미니 CNN인 RPN이 이미지 -&gt; CNN -&gt; output feature(feature map)를 잘라준다. RPN의 Covolution NN이 output feature를 sliding window방식으로 돌면서 연산후 classification 과 Regression 연산까지 한다. forward/ backward propagation -&gt; weight 업데이트 과정을 거치면 -&gt; Selective search를 대체하여 이미지를 2000개로 조각낸다. 즉, CNN기반의 RPN이 sliding window방식으로 box를 찾는 역활을 한다. RPN 구현 방법 이미지를 CNN으로 연산한다. 연산 결과를 n x n(보통 3x3) Convolutional Layer로 연산하고, 이 연산 결과가 맞는지를 보유하고 있는 bounding box 데이터와 Loss Function으로 비교한다. Loss function 결과로 backpropagation을 시키면 Region Proposal Network가 학습된다. 이 때, box를 찾는 과정에서, 어떤 object는 가로가 길고, 어떤 object는 세로가 길어서, sliding window가 꼭 정사각형이 아니라 직사각형 형태로 도는 것이 유리할 수 있다. 이러한 여러 형태의 sliding window를 anchor box라 한다. 그래서 RPN에서는 output feature인 feature map을 도는 여러개의 anchor box를 운영한다. 즉 CNN의 필터 대신, RPN은 anchor box를 사용하여, 따로 foward/backward하면서 training하여, 2000조각 낼 부분을 predict한다. Faster R-CNN구조 image를 CNN에 집어넣는다. CNN에서 나온 output feature(feature map)을 RPN에 집어넣어 classification 과 box를 얼마나 쳐야하는지를 따로 return받는다. Roi pooling을 이용하여 box크기를 fully-connected에 넣을 수 있게 resizing해준다. fast R-CNN과 동일하게 해준다. 1개의 모델에 끝에만 classification / regression을 따로 만들어 loss2개, weight업데이트도 2개로 따로하여 classification / regression(box위치)를 predict한다. ##TODO Mask R-CNN정리 참고http://incredible.ai/artificial-intelligence/2017/05/13/Transfer-Learning/ https://blog.lunit.io/2017/06/01/r-cnns-tutorial/ https://junn.in/archives/2517 https://nittaku.tistory.com/273]]></content>
      <categories>
        <category>theory</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mask RCNN-tensorflow tutorial]]></title>
    <url>%2F2019%2F03%2F27%2F2019-03-27-Mask-RCNN-tensorflow-tutorial%2F</url>
    <content type="text"><![CDATA[이 내용은 유튜버 Augmented Startups와 Mark Jay을 공부한 내용을 정리한 글입니다. Augmented Startups: https://www.youtube.com/watch?v=GSDbfGsxruA&amp;t=561s Mark Jay: https://www.youtube.com/watch?v=lLM8oAsi32g 1. Mask RCNN-tensorflow버전 설치Mask-RCNN-Tensorflow을 git clone 해준다1$ git clone https://github.com/matterport/Mask_RCNN.git Dependencies을 설치해주자먼저 필요한 python package들은 설치한다.12$ cd Mask_RCNN$ pip install -r requirements.txt 그리고 pretrained model을 다운받고 Mask-RCNN 폴더에 옮겨주자https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5 이제 cocoapi를 설치하자1234$ git clone https://github.com/philferriere/cocoapi.git$ cd cocoapi$ cd PythonAPI$ python setup.py build_ext install 설치가 잘되었는지를 확인하기터미널 창에 jupyter-notebook 입력해 쥬피터 노트북을 실행하고 samples 폴더안에 demo.jpynb를 실행시켜본다. 잘 설치된 경우 아래 그림과 같이 랜덤한 사진의 결과가 나온다. Failed to get convolution algorithm 문제 발생시: 12345import tensorflow as tfsess_config = tf.ConfigProto()sess_config.gpu_options.allow_growth = Truesess = tf.Session(config=sess_config) 와 같이 텐서플로우를 import하고 sess를 적절한위치에 추가해주자 2. webcam과 video으로 테스트 해보기webcam과 video 코드는 Mark Jay가 만든 코드를 사용할 것이다.https://github.com/markjay4k/Mask-RCNN-series 에서 visualize_cv2.py process_video.py 이 두개의 파일을 다운받는다. 그리고 visualize_cv2.py을 열어 다음과 같이 수정해준다. 12345678910111213import utils -&gt; from mrcnn import utilsimport model as modellib -&gt; from mrcnn import model as modellibROOT_DIR = os.getcwd() -&gt; ROOT_DIR = os.path.abspath("./")import coco -&gt; sys.path.append(os.path.join(ROOT_DIR,"samples/coco/"))import coco (위에 있던 import coco를 sys.path.append(os.path.join(ROOT_DIR,"samples/coco/")) 추가하고 아래에 옮겨준다. ) Failed to get convolution algorithm 문제 발생하면 위에 해결법을 이용하면 된다. 비디오 테스트에 경우 process_video.py에서 capture = cv2.VideoCapture(&#39;비디오 주소&#39;) 만 자신의 비디오 주소로 변경하면 된다. 3. 간단한 코드 분석123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133import cv2import numpy as npimport osimport sysfrom mrcnn import utilsfrom mrcnn import model as modellibimport tensorflow as tfROOT_DIR = os.path.abspath("./")MODEL_DIR = os.path.join(ROOT_DIR, "logs")sys.path.append(os.path.join(ROOT_DIR,"samples/coco/"))import cocoCOCO_MODEL_PATH = os.path.join(ROOT_DIR, "mask_rcnn_coco.h5")if not os.path.exists(COCO_MODEL_PATH): utils.download_trained_weights(COCO_MODEL_PATH)############# 여기까지 필요한 module importclass InferenceConfig(coco.CocoConfig): GPU_COUNT = 1 IMAGES_PER_GPU = 1config = InferenceConfig()config.display()############## cudnn 문제 해결 부분sess_config = tf.ConfigProto()sess_config.gpu_options.allow_growth = Truesess = tf.Session(config=sess_config)##############model = modellib.MaskRCNN( mode="inference", model_dir=MODEL_DIR, config=config ############# 모델 불러오기)model.load_weights(COCO_MODEL_PATH, by_name=True) ############# 모델 weight 불러오기class_names = [ 'BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']############## class이미지 이름 지정 (자리순서마다 이미어떤 클래스가 정해져 있고 이를 text로 표현하는 부분이다)def random_colors(N): np.random.seed(1) colors = [tuple(255 * np.random.rand(3)) for _ in range(N)] return colors############## class마다 다른 색깔로 segmentation 하기위한 부분colors = random_colors(len(class_names))class_dict = &#123; name: color for name, color in zip(class_names, colors)&#125;def apply_mask(image, mask, color, alpha=0.5): """apply mask to image""" for n, c in enumerate(color): image[:, :, n] = np.where( mask == 1, image[:, :, n] * (1 - alpha) + alpha * c, image[:, :, n] ) return image############## segemetation mask를 이미지에 표시def display_instances(image, boxes, masks, ids, names, scores): """ take the image and results and apply the mask, box, and Label """ n_instances = boxes.shape[0] if not n_instances: print('NO INSTANCES TO DISPLAY') else: assert boxes.shape[0] == masks.shape[-1] == ids.shape[0] for i in range(n_instances): if not np.any(boxes[i]): continue y1, x1, y2, x2 = boxes[i] label = names[ids[i]] color = class_dict[label] score = scores[i] if scores is not None else None caption = '&#123;&#125; &#123;:.2f&#125;'.format(label, score) if score else label mask = masks[:, :, i] image = apply_mask(image, mask, color) image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2) image = cv2.putText( image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2 ) return image############## mask, box, class_name을 모두 이미지에 표시if __name__ == '__main__': test everything capture = cv2.VideoCapture(0) # these 2 lines can be removed if you dont have a 1080p camera. capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640) capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 640) while True: ret, frame = capture.read() results = model.detect([frame], verbose=0) r = results[0] ############## 여기서 roi,masks, class_id가 나온다. frame = display_instances( frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'] ) cv2.imshow('frame', frame) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break capture.release()cv2.destroyAllWindows() 여기서 이용할 수 있는 부분은 results에서 나온 roi, masks, class_id로 어떤물체의 위치와 크기 이다. Mask-RCNN Train먼저 https://supervise.ly/이 사이트에 가입하고 사이트에 로그인한다. 그 후 Import 탭에 들어간 후 Import Plugin을 Images로 바꾸고 자신의 데이터셋 폴더를 위 그림의 상자안에 드래그 드랍 합니다. 그러면 위 사진처럼 창이 바뀌면 칸에 Project이름을 작성한후 Start Import 버튼을 누르면 Import가 됩니다. Project 탭에 들어가면 그림과 같이 자신이 Import한 이미지의 Project가 생성된 것을 확인할 수 있습니다. 이제 프로젝트를 클릭하고 이미지 폴더에 들어가면 다음과 같은 창이 열린다. 여기서 빨간색 박스가 쳐진 버튼을 클릭합니다. 그러면 이런 창이 뜨는데 여기서 Title에 물체의 label을 지정합니다. 그 후 그림처럼 labeling을 진행 합니다. 그리고 class를 하나더 추가하고 싶으면 위 그림처럼 Create Class를 눌러 추가 해주면됩니다. labeling을 다 했으면 다시 프로젝트 탭으로 돌아온후 위 그림처럼 Instance segmentation 버튼을 클릭해준다. Cluster 탭에 들어간 후 Instructions을 클릭합니다. 그러면 위 같은 창이 뜨는데 먼저 nvidia docker를 설치합니다. Neural Networks 탭에 들어간 후 ADD 버튼을 클릭합니다. 그 후 아래로 내려보면 Mask-RCNN이 있고 ADD버튼을 눌러준다. Neural Networks 탭에 다시 들어간 후 Train 버튼을 누른다. 자신의 데이터셋을 input project에 입력해주고 결과의 project이름을 정해준다. Train이 끝나면 다음과 같이 새로운 Neural Network가 생기고 여기서 Download를 해주거나 test버튼을 눌러 test 데이터를 test할 수 있다. 만약 Download한다면 .tar파일 안에 model.h5라는 파일 있는데 이 파일을 Mask-RCNN 폴더에 옮겨준다. 그리고 위 코드처럼 바꿔준다. 여기서 NUM_CLASSES 에서 뒤에 있는 2는 자신이 train 시킨 클래스의 수를 적어준다. https://deepmi.me/linux/18791/ nvidia docker를 설치 했으면 그림에 있는 명령어를 터미널에 입력합니다. TODO Mask-RCNN을 자신만의 데이터로 training 하기]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VNect-tensorflow]]></title>
    <url>%2F2019%2F03%2F22%2F2019-03-22-vnect-tensorflow-tutorial%2F</url>
    <content type="text"><![CDATA[VNect Tensorflow버전 github: https://github.com/timctho/VNect-tensorflow 1. 먼저 VNect-Tensorflow를 git clone 해준다.1$ git clone https://github.com/timctho/VNect-tensorflow.git 2. caffe python버전과 opengl을 설치한다.caffe는 여러가지 설치법이 있지만 anaconda에서 Python3로 쉽게 설치하는 방법은 아래 사이트를 참고한다 https://yangcha.github.io/Caffe-Conda3/ opengl의 경우는 1$ pip install PyOpenGL PyOpenGL_accelerate 으로 설치해주면 된다. 3. Vnect-Tensorflow가 설치된 폴더로 이동하고 caffe_weights_to_pickle.py를 실행한다.먼저 Vnect-Tensorflow가 설치된 폴더로 이동한다. 1$ cd VNect-tensorflow 그다음 환경을 자신이 caffe를 설치한 환경으로 바꿔준다. 1$ source activate testcaffe 이제 caffe로 만들어진 모델을 pickle 형식으로 바꿔준다. 123$ python caffe_weights_to_pickle.py --prototxt=../Documents/VNECT/mpii_vnect_model_code/mpii_vnect_model_demo/models/vnect_net.prototxt --caffemodel=../Documents/VNECT/mpii_vnect_model_code/mpii_vnect_model_demo/models/vnect_model.caffemodel 주소가 복잡하면 VNect-Tensorflow에 있는 models 폴더안에 vnect_net.prototxt 와 vnect_model.caffemodel 을 복사한후 아래와 같이 실행시키면 1$ python caffe_weights_to_pickle.py vnect.pkl 이라는 파일이 만들어졌을것이다. 4. models 폴더안에 vnect_model.py 수정하고 실행에 필요한 모델파일 만들기이유는 모르겠지만 직접 실행할때 필요한 모델을 만드는 파일이 없기에 models 폴더안에 vnect_model.py를 조금 수정해야한다.코드를 보시면 맨아래 if __name__ == &#39;name&#39;: 아래를 1234567model_file = '../vnect.pkl'model = VNect(368)with tf.Session() as sess: saver = tf.train.Saver() model.load_weights(sess, model_file) save_path = saver.save(sess, "./vnect_tf") 으로 바꿔주고 실행시키면 123vnect_tf.data-00000-of-00001vnect_tf.indexvnect_tf.meta 세가지 파일이 만들어 졌을것이다.이제 이 파일을 models/weights 안에 복사한다. (weights폴더가 없으니 만들어주자) 5. demo_tf_gl.py로 테스트 해보자.cudnn 오류때문에 1sess_config = tf.ConfigProto(device_count=gpu_count) 아래에 1sess_config.gpu_options.allow_growth = True 을 추가해준다. 그리고--demo_type&#39;, default=&#39;image&#39;를 --demo_type&#39;, default=&#39;webcam&#39; 으로 바꿔주면 웹캠으로 테스트가 가능하다. TODOcaffe 설치 오류 확인해보기]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
</search>
